{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIAT-SyM-Group/2025-dxpBGC-mining/blob/main/workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0a38522"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a3b9ff0"
      },
   "source": [
    "\n",
    "## Genome-mining workflow\n",
    "\n",
    "This notebook implements a reproducible genome-mining pipeline tailored for **biosynthetic gene cluster (BGC)** discovery:\n",
    "1. Provide one or more protein query sequences (FASTA or plain sequence).\n",
    "2. Run NCBI BLASTP (URLAPI) to obtain protein hits.\n",
    "3. Map protein hits to genomic context (IPG/nuccore), download flanking regions, and write local FASTA.\n",
    "4. Run antiSMASH on extracted neighborhoods.\n",
    "5. Build a local BGCs diamond database and run **cblaster** searches.\n",
    "6. Apply a rule-based filter and collect high-confidence candidate regions for downstream analysis.\n",
    "\n",
    "**How to run:** fill in the forms below, then go to `Runtime → Run all`.\n",
    "\n",
    "> Notes\n",
    "> - NCBI **email is required** for Entrez.\n",
    "> - Set an **NCBI API key** for higher rate limits (recommended).\n",
    "> - The pipeline is written to be Colab-friendly: no manual shell scripting is required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "f9568aac",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5c575faa-9af5-4c34-95cc-7765099272c0"
   },
   "source": [
    "#@title 1) Install Python dependencies and configure NCBI credentials\n",
    "#@markdown Provide NCBI credentials here (email is required; API key is optional but recommended for higher rate limits).\n",
    "#@markdown\n",
    "#@markdown Options:\n",
    "#@markdown - Fill the form fields below, or\n",
    "#@markdown - Set environment variables NCBI_EMAIL / NCBI_API_KEY before running.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pkgutil\n",
    "from typing import Optional\n",
    "\n",
    "def _pip_install(*pkgs: str) -> None:\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs]\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def ensure_import(pkg: str, import_name: Optional[str] = None, pip_name: Optional[str] = None) -> None:\n",
    "    name = import_name or pkg\n",
    "    if pkgutil.find_loader(name) is None:\n",
    "        _pip_install(pip_name or pkg)\n",
    "\n",
    "# Core dependencies used across cells\n",
    "ensure_import(\"requests\")\n",
    "ensure_import(\"tqdm\", \"tqdm\")\n",
    "ensure_import(\"biopython\", \"Bio\", \"biopython\")\n",
    "\n",
    "# Optional convenience dependencies (uncomment if needed)\n",
    "# ensure_import(\"pandas\")\n",
    "# ensure_import(\"numpy\")\n",
    "\n",
    "# -------------------- Credential configuration --------------------\n",
    "NCBI_EMAIL: str = \"\"  #@param {type:\"string\"}\n",
    "NCBI_API_KEY: str = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "# Fallback to CONTACT_EMAIL from cell 0, if available\n",
    "if not NCBI_EMAIL:\n",
    "    NCBI_EMAIL = globals().get(\"EMAIL\", \"\").strip()\n",
    "\n",
    "if not NCBI_EMAIL:\n",
    "    raise ValueError(\"NCBI_EMAIL is required. Set it in this cell or via the NCBI_EMAIL environment variable.\")\n",
    "\n",
    "os.environ[\"NCBI_EMAIL\"] = NCBI_EMAIL\n",
    "if NCBI_API_KEY:\n",
    "    os.environ[\"NCBI_API_KEY\"] = NCBI_API_KEY\n",
    "\n",
    "# Configure Biopython Entrez defaults (used in later cells)\n",
    "from Bio import Entrez\n",
    "Entrez.email = NCBI_EMAIL\n",
    "if NCBI_API_KEY:\n",
    "    Entrez.api_key = NCBI_API_KEY\n",
    "\n",
    "print(\"[INFO] Python deps: OK\")\n",
    "print(f\"[INFO] NCBI_EMAIL set: {NCBI_EMAIL}\")\n",
    "print(f\"[INFO] NCBI_API_KEY set: {'YES' if bool(NCBI_API_KEY) else 'NO'}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "c701a4c0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "outputId": "bdfd7245-ab39-402d-d3b1-176fc4c27974"
   },
   "source": [
    "#@title 2) Provide protein query sequence(s) and write a standardized FASTA\n",
    "#@markdown Paste either a raw amino-acid sequence (no header) or a FASTA (starting with '>').\n",
    "#@markdown - **jobname** is used as the FASTA record ID when a raw sequence is provided.\n",
    "#@markdown - The output FASTA (**out_fasta**) is used by the subsequent BLASTP cell.\n",
    "\n",
    "import os, re, time, threading, xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import Entrez, SeqIO\n",
    "\n",
    "query_sequence = \"MSHTPSQPQPSTDKKADKRLNKRLKDMPIAIVGMASIFANSRYLNKFWDLICDKIDAITDVPASHWAIDDYYDVDKSKADKSYCKRGGFMPEVDFNPMEFGLPPNILELTDSSQLLSLVVAKEVLQDANLPDDYDRDRIGITLGIGGGQKLSHSLNARLQYPVLKKVFKSSGLSDEDSELLIKKFQDQYVHWEENSFPGSLGNVIAGRIANRFDLGGMNCVVDAACAGSLAAMRMALTELTEGRSDMMITGGVCTDNSPYMYMSFSKTPAFTTNEQIQPFDIDSKGMMIGEGIGMVALKRLDDAERDGDRIYAVIKGVGASSDGKFKSIYAPRPEGQAKALERAYDDAGFAPHTVGLIEAHGTGTAAGDVAEFTGLSSVFSQDNAQLQHIALGSVKSQVGHTKSTAGTAGVIKAALALHHKVLPPTINVSKPNPKLEIDRSPFYLNTEARPWIQRSDDTPRRAGISSFGFGGTNFHLVLEEYRPDHTRDDAYRQRSVAQILLFAANDKTLLLNELKAVLQQASSAKAELSEAHFIQFAKPYALREITPQSARLGFIAKDYAQLQTLLTQAIAQLEANNAESWQLPSGISYRAKALVNEQTKIAALFAGQGAQYLNMGLELANNFPELRRHIHASDKVFSTHGKPALSSVLYPIPAFDDESIKAQETALTNTLYAQSAIGALSMAQYALFTQAGFAPDMLAGHSFGELSALCAAGVISMDDYIKLAFERGQAMAQSSQDTDAGVMYAVILKQKQDIEVINGCLAQFEGVKVANYNSPTQLVIAGASAATQQAAKAISELGFKAIALPVSGAFHTPLVAHAQKPFSAAIDKAQFNTPKIALYANGTGQLHPIDANAIKAALKDHMLQSVHFSEQLEAMYAAGARVFVEFGPKNILQKLTENTLAAQLNELCIISINPNPKGDSDSQLRSAAVQLAVAGVKLREIDPYQAELIAPAATSAMNIKLNATNYISPATRSKMVDSLQSGKITSQVQYVDRIVEKVVEKVVEKPVIVEKILEKVVEVEKPVAQNSNNIQQQTPAQPASFTAGQTNQDALSAFFAAQTQAAQLHQQFLAIPQQYGDTVSALMAEQAKMASLGIAIPESLQRSMELFHQHQAQTLKSHSDFMQLQTSSSQAVLALLGQMPASQVQAPIQAAAPVAVAVTKPVVPAQAPVVQGLAAEPKVTAVPVSEPTVQQPQVALAQVAQTKVTQPPLAQPQVQTVAAQTSALQVKPALQQIEHAMLSVVADKTGYPVEMLELSMDMEADLGIDSIKRVEILGTVQDELPNLPELSPEDLAECRTLGEIVALFSQAAPVTSATTVSHATQSAVAASAAVSNDEIERTMMAVVADKTGYPVEMLELSMDMEADLGIDSIKRVEILGTVQDELPNLPELSPEDLAECRTLGEIVALFSQAVPVAAQTFAAMAATNPQVVASAVTPIAAVSDGEIEHTMMAVVADKTGYPVEMLELSMDMEADLGIDSIKRVEILGTVQDKLPNLPELSPEDLAECRTLGEIAALFSQAAPVTAAATVSHATQSAIAARAAVSNDEIERTMMAVVADKTGYPVEMLELSMDMEADLGIDSIKRVEILGTVQDQLPNLPELSPEDLAECRTLGEIVALYAGSQSSSEALQQNHAATIQETQEAIAKTVEETIDLPPHSEVMLKKLPAAAELARIIATSDVQLTANSYVVIGDDGHNAGVIAEKLHAQGVKVAVVRSPKTVVTSASPLDSHIASFTLEAIDDESICEVINQIEALGQIAGFIHLQPQHKSVADKGAGLVLVDEAKASVEQAFLFAKFLQPLLTERDYCRFVTVSCIDGGFGYIGMDESVGALISQSELNQAALFGLTKTLNHEWPGVVCRALDIAPNLDAKTVANAVVQEYYLQDAPVEVGIDSNFDRVTLVAGTAALRHPPAVLSNADKILVTGGAKGVTFECALSLAKRCQAHFFLAGRSAHQVIPAWAEGKKSNELKAAAIAHLQSLGDKPTPKQVDALVWPVQSSLEISHALAAFDAIGASAEYLSVDVNDPAAIASTIAPINALSPITGIIHGAGVLADKHIQDKTLNEFERVYGTKVTGLNNLLSTLDLSQVKLIALFSSAAGFYGNTGQSDYAMSNDILNKAALQLAQQLPQAKVMSFDWGPWDGGMVNPALKKMFIDRGVYVIPLKAGAELFASQLLSDTGAQLLVGTDMQGNTANAVEVASAKKPEADLATALDPQPMAQTVPQSIRVMRSLDPKRMSFIEDHCINGHAVLPTVCAIDWMREAAKAHLGTAVSVSDYRLLKGVIFDEALLARNAPIELELMLTPLADAAQQSTEALAALISFEGRPQYQAVLVAQTDDMPDAQRFEVGELHSLIQEMAQQPAIANRESLYSDGTLFHGPRLQGISEVLTFDDQHLMAKVELPQVALDDCGKFAPKLEDKGTQPFAEDLLLQAMLVWARLKYQAASLPSTIGEFVSYAPLSFGEKAVLVLDVLKHSSRSLEANIALYHQDGRLSCEMKRAKVTISKTLNQAFLANKPQQLAQVQASIQNMAEVSVK\"  #@param {type:\"string\", multiline:true}\n",
    "jobname = \"AAN54658.1-PfaA\"  #@param {type:\"string\"}\n",
    "out_fasta = \"AAN54658.1-PfaA.faa\"  #@param {type:\"string\"}\n",
    "wrap_width = 80  #@param {type:\"integer\"}\n",
    "\n",
    "import re, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "def _clean_seq(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    return s.upper()\n",
    "\n",
    "def _sanitize_id(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        raise ValueError(\"jobname must not be empty (it is used as the FASTA Query ID).\")\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = s.replace(\">\", \"_\")\n",
    "    return s\n",
    "\n",
    "def write_standard_fasta(query_sequence: str, jobname: str, out_fasta: str, wrap_width: int = 60):\n",
    "    out_path = Path(out_fasta)\n",
    "\n",
    "    qs = (query_sequence or \"\").strip()\n",
    "    if not qs:\n",
    "        raise ValueError(\"query_sequence must not be empty.\")\n",
    "\n",
    "    if re.search(r\"^\\s*>\", qs, flags=re.MULTILINE):\n",
    "        lines = qs.splitlines()\n",
    "        records = []\n",
    "        cur_h = None\n",
    "        cur_s = []\n",
    "        for ln in lines:\n",
    "            if ln.startswith(\">\"):\n",
    "                if cur_h is not None:\n",
    "                    seq = _clean_seq(\"\".join(cur_s))\n",
    "                    records.append((cur_h, seq))\n",
    "                cur_h = ln.strip()\n",
    "                cur_s = []\n",
    "            else:\n",
    "                cur_s.append(ln.strip())\n",
    "        if cur_h is not None:\n",
    "            seq = _clean_seq(\"\".join(cur_s))\n",
    "            records.append((cur_h, seq))\n",
    "\n",
    "        if not records:\n",
    "            raise ValueError(\"Detected '>' but failed to parse a valid FASTA record.\")\n",
    "\n",
    "        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for h, seq in records:\n",
    "                f.write(h + \"\\n\")\n",
    "                f.write(\"\\n\".join(textwrap.wrap(seq, width=int(wrap_width))) + \"\\n\")\n",
    "\n",
    "        preview_header = records[0][0]\n",
    "        preview_len = len(records[0][1])\n",
    "\n",
    "    else:\n",
    "        qid = _sanitize_id(jobname)\n",
    "        seq = _clean_seq(qs)\n",
    "        if not seq:\n",
    "            raise ValueError(\"Sequence becomes empty after cleaning. Please confirm you pasted a valid sequence.\")\n",
    "\n",
    "        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\">{qid}\\n\")\n",
    "            f.write(\"\\n\".join(textwrap.wrap(seq, width=int(wrap_width))) + \"\\n\")\n",
    "\n",
    "        preview_header = f\">{qid}\"\n",
    "        preview_len = len(seq)\n",
    "\n",
    "    print(f\"[OK] Wrote standard FASTA: {out_path.resolve()}\")\n",
    "    print(f\"[OK] First record: {preview_header}  (length={preview_len})\")\n",
    "    print(\"\\n--- Preview (first 20 lines) ---\")\n",
    "    print(\"\\n\".join(out_path.read_text(encoding=\"utf-8\").splitlines()[:20]))\n",
    "\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(str(out_path))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "write_standard_fasta(query_sequence, jobname, out_fasta, wrap_width)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "cea10bc6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6a2c9f99-ac65-4832-cfc3-f1cd990f8b38"
   },
   "source": [
    "#@title 3) Run BLASTP via NCBI BLAST URLAPI (PROGRAM is fixed to blastp)\n",
    "#@markdown This cell submits the query FASTA to NCBI BLAST URLAPI, polls for completion, and downloads tabular results.\n",
    "#@markdown It automatically uses **out_fasta** from the previous cell when available.\n",
    "\n",
    "\n",
    "import os, time, re\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "\n",
    "BLAST_URL = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n",
    "\n",
    "PROGRAM = \"blastp\"\n",
    "\n",
    "DATABASE   = \"refseq_protein\"  #@param {type:\"string\"}\n",
    "HIT_TOP_N  = 10              #@param {type:\"integer\"}\n",
    "EXPECT     = \"1e-5\"            #@param {type:\"string\"}\n",
    "\n",
    "POLL_SEC   = 60\n",
    "MAX_WAIT_H = 6\n",
    "CLEAN_OLD  = True              #@param {type:\"boolean\"}\n",
    "\n",
    "TIMEOUT_PUT = 180              #@param {type:\"integer\"}\n",
    "TIMEOUT_GET = 300              #@param {type:\"integer\"}\n",
    "# ==============================\n",
    "\n",
    "query_fa = globals().get(\"out_fasta\", globals().get(\"query_fa\", \"query.faa\"))\n",
    "jobname  = globals().get(\"jobname\", \"job\")\n",
    "EMAIL    = os.environ.get(\"NCBI_EMAIL\", globals().get(\"EMAIL\", \"\")).strip()\n",
    "TOOL     = globals().get(\"TOOL\", \"symlab_notebook\")\n",
    "\n",
    "N = int(HIT_TOP_N)\n",
    "MAX_WAIT = int(MAX_WAIT_H) * 3600\n",
    "\n",
    "outf = f\"{PROGRAM}.top{N}.tsv\"\n",
    "logf = f\"{PROGRAM}.urlapi.log\"\n",
    "rawf = f\"{PROGRAM}.last_raw.txt\"\n",
    "ridf = f\"{PROGRAM}.rid.txt\"\n",
    "\n",
    "def log(line: str):\n",
    "    with open(logf, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip(\"\\n\") + \"\\n\")\n",
    "\n",
    "def tail_fast(path, n=25, block=8192):\n",
    "    if not os.path.exists(path):\n",
    "        return \"(log not created yet)\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        end = f.tell()\n",
    "        data = b\"\"\n",
    "        pos = end\n",
    "        while pos > 0 and data.count(b\"\\n\") <= n:\n",
    "            step = min(block, pos)\n",
    "            pos -= step\n",
    "            f.seek(pos)\n",
    "            data = f.read(step) + data\n",
    "    return \"\\n\".join(data.decode(\"utf-8\", errors=\"replace\").splitlines()[-n:])\n",
    "\n",
    "def status_screen(phase: str, extra: str = \"\"):\n",
    "    size = os.path.getsize(outf) if os.path.exists(outf) else 0\n",
    "    clear_output(wait=True)\n",
    "    print(time.strftime(\"%F %T\"))\n",
    "    print(f\"phase: {phase}\")\n",
    "    print(f\"query_fa: {query_fa}\")\n",
    "    print(f\"db/program: {DATABASE} / {PROGRAM}\")\n",
    "    print(f\"N/evalue: {N} / {EXPECT}\")\n",
    "    print(f\"out: {size/1024/1024:.2f} MB  ({outf})\")\n",
    "    if extra:\n",
    "        print(extra)\n",
    "    print(\"\\n--- last log lines ---\")\n",
    "    print(tail_fast(logf, n=25))\n",
    "\n",
    "def parse_rid_rtoe(text: str):\n",
    "    m_rid  = re.search(r\"\\bRID\\s*=\\s*([A-Z0-9]+)\\b\", text)\n",
    "    m_rtoe = re.search(r\"\\bRTOE\\s*=\\s*(\\d+)\\b\", text)\n",
    "    rid  = m_rid.group(1) if m_rid else None\n",
    "    rtoe = int(m_rtoe.group(1)) if m_rtoe else None\n",
    "    return rid, rtoe\n",
    "\n",
    "def parse_status_hits(text: str):\n",
    "    ms = re.search(r\"Status=([A-Z]+)\", text)\n",
    "    mh = re.search(r\"ThereAreHits=([a-z]+)\", text)\n",
    "    return (ms.group(1) if ms else \"NA\"), (mh.group(1) if mh else \"NA\")\n",
    "\n",
    "def extract_pre(text: str) -> str:\n",
    "    m = re.search(r\"<PRE>(.+?)</PRE>\", text, re.DOTALL | re.IGNORECASE)\n",
    "    return m.group(1) if m else text\n",
    "\n",
    "def tabular_lines(text: str):\n",
    "    out = []\n",
    "    for line in text.splitlines():\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.count(\"\\t\") >= 11:\n",
    "            out.append(line)\n",
    "    return out\n",
    "\n",
    "def looks_like_tabular(lines):\n",
    "    for ln in lines[:50]:\n",
    "        cols = ln.split(\"\\t\")\n",
    "        if len(cols) < 12:\n",
    "            continue\n",
    "        try:\n",
    "            float(cols[2])   # pident\n",
    "            float(cols[10])  # evalue\n",
    "            float(cols[11])  # bitscore\n",
    "            return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "if not os.path.exists(query_fa):\n",
    "    raise FileNotFoundError(\n",
    "        f\"FASTA file not found: {query_fa}\\n\"\n",
    "        f\"Run the previous cell to generate the FASTA (or set out_fasta/query_fa to an existing FASTA file).\"\n",
    "    )\n",
    "\n",
    "if CLEAN_OLD:\n",
    "    for f in (outf, logf, rawf, ridf):\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "\n",
    "with open(query_fa, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    fasta = f.read().strip()\n",
    "if not fasta.startswith(\">\"):\n",
    "    raise ValueError(f\"{query_fa} is not a valid FASTA (must start with '>').\")\n",
    "\n",
    "sess = requests.Session()\n",
    "sess.headers.update({\"User-Agent\": TOOL})\n",
    "t0 = time.time()\n",
    "\n",
    "# 1) SUBMIT (CMD=Put)\n",
    "phase = \"SUBMIT (CMD=Put)\"\n",
    "status_screen(phase, f\"jobname={jobname}\\nEMAIL={EMAIL}\\nTOOL={TOOL}\")\n",
    "log(f\"[{time.strftime('%F %T')}] {phase}\")\n",
    "\n",
    "put_params = {\n",
    "    \"CMD\": \"Put\",\n",
    "    \"PROGRAM\": PROGRAM,\n",
    "    \"DATABASE\": DATABASE,\n",
    "    \"EXPECT\": EXPECT,\n",
    "    \"HITLIST_SIZE\": str(N),\n",
    "    \"ALIGNMENTS\": str(N),\n",
    "    \"DESCRIPTIONS\": str(N),\n",
    "    \"EMAIL\": EMAIL,\n",
    "    \"TOOL\": TOOL,\n",
    "}\n",
    "\n",
    "r = sess.post(BLAST_URL, params=put_params, files={\"QUERY\": (None, fasta)}, timeout=TIMEOUT_PUT)\n",
    "r.raise_for_status()\n",
    "\n",
    "RID, RTOE = parse_rid_rtoe(r.text)\n",
    "if not RID:\n",
    "    with open(rawf, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        f.write(r.text)\n",
    "    log(\"PUT response saved -> \" + rawf)\n",
    "    raise RuntimeError(\"Failed to parse RID (PUT response was written to rawf).\")\n",
    "\n",
    "with open(ridf, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(RID + \"\\n\")\n",
    "\n",
    "log(f\"[{time.strftime('%F %T')}] RID={RID} RTOE={RTOE if RTOE is not None else 'NA'}\")\n",
    "status_screen(\"SUBMITTED\", f\"RID={RID}\\nRID saved -> {ridf}\\nRTOE={RTOE if RTOE is not None else 'NA'}\")\n",
    "\n",
    "if RTOE:\n",
    "    for left in range(RTOE, 0, -1):\n",
    "        if left % 3 == 0:\n",
    "            status_screen(\"WAIT (RTOE)\", f\"RID={RID}  left={left}s\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# 2) POLL\n",
    "deadline = t0 + MAX_WAIT\n",
    "tries = 0\n",
    "\n",
    "while True:\n",
    "    tries += 1\n",
    "    if time.time() > deadline:\n",
    "        raise TimeoutError(f\"Timed out after MAX_WAIT={MAX_WAIT}s waiting for READY; RID={RID}\")\n",
    "\n",
    "    phase = \"POLL (SearchInfo)\"\n",
    "    params = {\"CMD\": \"Get\", \"RID\": RID, \"FORMAT_OBJECT\": \"SearchInfo\", \"EMAIL\": EMAIL, \"TOOL\": TOOL}\n",
    "    res = sess.get(BLAST_URL, params=params, timeout=TIMEOUT_GET)\n",
    "    res.raise_for_status()\n",
    "    body = res.text\n",
    "\n",
    "    with open(rawf, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        f.write(body)\n",
    "\n",
    "    status, hitsflag = parse_status_hits(body)\n",
    "    log(f\"[{time.strftime('%F %T')}] SearchInfo: Status={status} ThereAreHits={hitsflag} tries={tries}\")\n",
    "\n",
    "    if status in (\"FAILED\", \"UNKNOWN\"):\n",
    "        status_screen(\"FAILED\", f\"RID={RID}\\nStatus={status}\\nraw saved -> {rawf}\")\n",
    "        raise RuntimeError(f\"Search {RID} failed (Status={status}).\")\n",
    "\n",
    "    if status == \"READY\" and hitsflag == \"no\":\n",
    "        status_screen(\"NO_HITS\", f\"RID={RID}\\nStatus=READY ThereAreHits=no\\nraw saved -> {rawf}\")\n",
    "        raise RuntimeError(f\"Search completed but found no hits (RID={RID}).\")\n",
    "\n",
    "    if status == \"READY\" and hitsflag == \"yes\":\n",
    "        status_screen(\"READY\", f\"RID={RID}\\nStatus=READY ThereAreHits=yes\\ntries={tries}\")\n",
    "        break\n",
    "\n",
    "    status_screen(phase, f\"RID={RID}\\ntry={tries}\\nStatus={status}  ThereAreHits={hitsflag}\\nraw saved -> {rawf}\\nnext poll in {POLL_SEC}s\")\n",
    "    time.sleep(max(1, int(POLL_SEC)))\n",
    "\n",
    "# 3) DOWNLOAD (Tabular)\n",
    "phase = \"DOWNLOAD (Tabular)\"\n",
    "log(f\"[{time.strftime('%F %T')}] {phase} RID={RID}\")\n",
    "status_screen(phase, f\"RID={RID}\\nRequesting Tabular...\")\n",
    "\n",
    "get_params = {\n",
    "    \"CMD\": \"Get\",\n",
    "    \"RID\": RID,\n",
    "    \"FORMAT_OBJECT\": \"Alignment\",\n",
    "    \"FORMAT_TYPE\": \"Text\",\n",
    "    \"ALIGNMENT_VIEW\": \"Tabular\",\n",
    "    \"HITLIST_SIZE\": str(N),\n",
    "    \"ALIGNMENTS\": str(N),\n",
    "    \"DESCRIPTIONS\": str(N),\n",
    "    \"EMAIL\": EMAIL,\n",
    "    \"TOOL\": TOOL,\n",
    "}\n",
    "\n",
    "res = sess.get(BLAST_URL, params=get_params, timeout=TIMEOUT_GET)\n",
    "res.raise_for_status()\n",
    "raw = res.text\n",
    "\n",
    "with open(rawf, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    f.write(raw)\n",
    "\n",
    "text = extract_pre(raw)\n",
    "lines = tabular_lines(text)\n",
    "\n",
    "if not looks_like_tabular(lines):\n",
    "    status_screen(\"BAD_OUTPUT\", f\"RID={RID}\\nNo valid tabular (outfmt 6) lines were detected.\\nraw saved -> {rawf}\")\n",
    "    raise RuntimeError(\"Downloaded content does not look like tabular results; raw response was saved to rawf for inspection.\")\n",
    "\n",
    "with open(outf, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ln in lines:\n",
    "        f.write(ln + \"\\n\")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "log(f\"[{time.strftime('%F %T')}] DONE RID={RID} tries={tries} rows={len(lines)} bytes={os.path.getsize(outf)} elapsed={elapsed:.1f}s\")\n",
    "status_screen(\"DONE\", f\"RID={RID}\\ntries={tries}\\nrows={len(lines)}\\nelapsed={elapsed:.1f}s\\nwrote: {outf}\\nraw saved -> {rawf}\")\n",
    "print(\"\\n[FINISHED]\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "e33fbef4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344,
     "referenced_widgets": [
      "09f0466485e642f9867d680e6854ec66",
      "364bc977888843d78cc86b6fabe14705",
      "1cb7fc53359d4f85965d994a1c86410d",
      "b2cfae048f454175906e3c5d8dcf445a",
      "051713e22d004f8584c9d42c6a857f09",
      "da4a5e6623184bef9ec23891a9a69ef3",
      "157b5e55672a4641b2c6cb829a083c5a",
      "9702c65af22d4b27b55d20c917c74444",
      "5ad773c9a0614b418fe77f7766f43636",
      "0d0221543b9c4e819ebfba1fbec2261a",
      "9f028ae5b5b84982ade9d92abfa98ebe"
     ]
    },
    "outputId": "e389c26d-87cc-488b-82d7-5b3d8fb75d1d"
   },
   "source": [
    "#@title 4) Map BLAST hits to genomes (IPG) and download ±FLANK nuccore neighborhoods (FASTA)\n",
    "#@markdown Inputs: the BLASTP TSV produced in the previous cell (variable **outf**), or a file path you specify below.\n",
    "#@markdown Output: per-hit neighborhood FASTA files under **wp_flanks_fna/**.\n",
    "\n",
    "\n",
    "# ---- user-configurable params ----\n",
    "FLANK = 100000 #@param {type:\"integer\"}\n",
    "THREADS = 10 #@param {type:\"integer\"}\n",
    "\n",
    "TOP_N_UNIQ_TAXID = 1 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "# ---- imports ----\n",
    "import os\n",
    "NCBI_EMAIL = os.environ.get('NCBI_EMAIL', globals().get('EMAIL',''))\n",
    "NCBI_API_KEY = os.environ.get('NCBI_API_KEY','')\n",
    "\n",
    "import re, time, threading, xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import Entrez, SeqIO\n",
    "\n",
    "# -------------------- AUTO / FIXED CONFIG --------------------\n",
    "blast_tsv = globals().get(\"outf\", \"blastp.top5000.tsv\")\n",
    "\n",
    "outdir = Path(\"wp_flanks_fna\")\n",
    "wp_list_txt = Path(\"wp.unique.col2.txt\")\n",
    "picked_list_txt = Path(\"picked.unique_taxid.items.tsv\")\n",
    "\n",
    "PREFER_REFSEQ = True\n",
    "DO_ORG_LOOKUP = True\n",
    "\n",
    "NCBI_TOOL    = \"symlab_wp_flanks_notebook\"\n",
    "ENTREZ_MAX_TRIES = 5\n",
    "ENTREZ_SLEEP_BETWEEN_TRIES = 180\n",
    "ENTREZ_TIMEOUT = 20\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ---- validate ----\n",
    "NCBI_EMAIL = (NCBI_EMAIL or \"\").strip()\n",
    "NCBI_API_KEY = (NCBI_API_KEY or \"\").strip()\n",
    "if not NCBI_EMAIL:\n",
    "    raise ValueError(\"NCBI_EMAIL is required for NCBI Entrez access.\")\n",
    "if THREADS < 1:\n",
    "    raise ValueError(\"THREADS must be >= 1\")\n",
    "if TOP_N_UNIQ_TAXID < 0:\n",
    "    raise ValueError(\"TOP_N_UNIQ_TAXID must be >= 0 (0 means no limit)\")\n",
    "\n",
    "# ---- configure Entrez ----\n",
    "Entrez.email = os.environ.get('NCBI_EMAIL', globals().get('EMAIL', ''))\n",
    "Entrez.api_key = os.environ.get('NCBI_API_KEY', '') or None\n",
    "Entrez.tool = NCBI_TOOL\n",
    "Entrez.max_tries = ENTREZ_MAX_TRIES\n",
    "Entrez.sleep_between_tries = ENTREZ_SLEEP_BETWEEN_TRIES\n",
    "Entrez.timeout = ENTREZ_TIMEOUT\n",
    "\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def normalize_wp(s: str):\n",
    "    \"\"\"Extract WP_XXXXX.Y from BLAST sseqid\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"\\b(WP_\\d+\\.\\d+)\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    for tok in s.split(\"|\"):\n",
    "        tok = tok.strip()\n",
    "        if re.fullmatch(r\"WP_\\d+\\.\\d+\", tok):\n",
    "            return tok\n",
    "    return None\n",
    "\n",
    "def _clean_for_filename(name: str, max_len: int = 120) -> str:\n",
    "    name = (name or \"\").strip()\n",
    "    name = re.sub(r\"[^\\w\\.\\-]+\", \"_\", name)\n",
    "    name = re.sub(r\"_+\", \"_\", name).strip(\"_\")\n",
    "    if not name:\n",
    "        name = \"Unknown_organism\"\n",
    "    if len(name) > max_len:\n",
    "        name = name[:max_len].rstrip(\"_\")\n",
    "    return name\n",
    "\n",
    "def _clean_text(name: str) -> str:\n",
    "    name = (name or \"\").strip()\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Global rate limit: ~3 rps without an API key; ~10 rps with an API key (more stable with threads).\"\"\"\n",
    "    def __init__(self, rps: float):\n",
    "        self.min_interval = 1.0 / max(0.1, rps)\n",
    "        self._lock = threading.Lock()\n",
    "        self._next_time = 0.0\n",
    "    def wait(self):\n",
    "        with self._lock:\n",
    "            now = time.monotonic()\n",
    "            if now < self._next_time:\n",
    "                time.sleep(self._next_time - now)\n",
    "            self._next_time = time.monotonic() + self.min_interval\n",
    "\n",
    "rps = 10.0 if NCBI_API_KEY else 3.0\n",
    "RL = RateLimiter(rps=rps)\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "RETRY_SLEEP = 5\n",
    "\n",
    "def entrez_efetch_limited(**kwargs):\n",
    "    last_err = None\n",
    "    for i in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            RL.wait()\n",
    "            return Entrez.efetch(**kwargs)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if i == MAX_RETRIES:\n",
    "                raise\n",
    "            time.sleep(RETRY_SLEEP)\n",
    "    raise last_err\n",
    "\n",
    "@dataclass\n",
    "class CdsMapping:\n",
    "    wp: str\n",
    "    accver: str\n",
    "    start: int\n",
    "    stop: int\n",
    "    strand: str\n",
    "    taxid: str = \"\"\n",
    "    org: str = \"\"\n",
    "\n",
    "_ipg_cache_lock = threading.Lock()\n",
    "_ipg_cache: dict[str, list[CdsMapping]] = {}\n",
    "\n",
    "def fetch_ipg_mappings_for_wp(wp: str) -> list[CdsMapping]:\n",
    "    with _ipg_cache_lock:\n",
    "        if wp in _ipg_cache:\n",
    "            return _ipg_cache[wp]\n",
    "\n",
    "    with entrez_efetch_limited(db=\"protein\", id=wp, rettype=\"ipg\", retmode=\"xml\") as h:\n",
    "        xml_bytes = h.read()\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    out: list[CdsMapping] = []\n",
    "    for cds in root.iter(\"CDS\"):\n",
    "        accver = cds.attrib.get(\"accver\", \"\")\n",
    "        s = cds.attrib.get(\"start\", \"\")\n",
    "        t = cds.attrib.get(\"stop\", \"\")\n",
    "        if not (accver and s and t):\n",
    "            continue\n",
    "        try:\n",
    "            start = int(s)\n",
    "            stop  = int(t)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        out.append(CdsMapping(\n",
    "            wp=wp,\n",
    "            accver=accver,\n",
    "            start=start,\n",
    "            stop=stop,\n",
    "            strand=cds.attrib.get(\"strand\", \"?\"),\n",
    "            taxid=cds.attrib.get(\"taxid\", \"\") or \"\",\n",
    "            org=cds.attrib.get(\"org\", \"\") or \"\",\n",
    "        ))\n",
    "\n",
    "    with _ipg_cache_lock:\n",
    "        _ipg_cache[wp] = out\n",
    "    return out\n",
    "\n",
    "def choose_representative_mapping(mappings: list[CdsMapping]) -> CdsMapping | None:\n",
    "    if not mappings:\n",
    "        return None\n",
    "\n",
    "    cand = mappings\n",
    "    if PREFER_REFSEQ:\n",
    "        refseq = [m for m in cand if m.accver.startswith((\"NC_\", \"NZ_\"))]\n",
    "        if refseq:\n",
    "            cand = refseq\n",
    "\n",
    "    for m in cand:\n",
    "        if (m.taxid or \"\").strip():\n",
    "            return m\n",
    "    return cand[0]\n",
    "\n",
    "def fetch_nuccore_fasta_segment(accver: str, seg_start: int, seg_end: int):\n",
    "    with entrez_efetch_limited(\n",
    "        db=\"nuccore\",\n",
    "        id=accver,\n",
    "        rettype=\"fasta\",\n",
    "        retmode=\"text\",\n",
    "        seq_start=seg_start,\n",
    "        seq_stop=seg_end,\n",
    "    ) as h:\n",
    "        return SeqIO.read(h, \"fasta\")\n",
    "\n",
    "def _get_organism_from_gb(acc: str, seg_start: int, seg_end: int) -> str:\n",
    "    try:\n",
    "        with entrez_efetch_limited(\n",
    "            db=\"nuccore\",\n",
    "            id=acc,\n",
    "            rettype=\"gb\",\n",
    "            retmode=\"text\",\n",
    "            seq_start=seg_start,\n",
    "            seq_stop=seg_end,\n",
    "        ) as h:\n",
    "            gb_rec = SeqIO.read(h, \"gb\")\n",
    "        org = gb_rec.annotations.get(\"organism\") or \"\"\n",
    "        if not org:\n",
    "            for feat in getattr(gb_rec, \"features\", []):\n",
    "                if feat.type == \"source\":\n",
    "                    org = feat.qualifiers.get(\"organism\", [\"\"])[0]\n",
    "                    if org:\n",
    "                        break\n",
    "        return org or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _fallback_org_from_fasta_desc(record) -> str:\n",
    "    desc = record.description or \"\"\n",
    "    if record.id and desc.startswith(record.id):\n",
    "        desc = desc[len(record.id):].strip()\n",
    "    desc = re.sub(r\"bases\\s+\\d+\\s+to\\s+\\d+\", \"\", desc, flags=re.I).strip()\n",
    "    m = re.match(r\"(.+?)(?:,|$)\", desc)\n",
    "    return (m.group(1).strip() if m else \"\").strip()\n",
    "\n",
    "_file_locks = defaultdict(threading.Lock)\n",
    "\n",
    "def append_record_to_file(path: Path, fasta_text: str):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    lock = _file_locks[str(path)]\n",
    "    with lock:\n",
    "        with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(fasta_text)\n",
    "            if not fasta_text.endswith(\"\\n\"):\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "_accver_meta_lock = threading.Lock()\n",
    "_accver_meta_cache: dict[str, tuple[str, str]] = {}  # accver -> (organism, taxid)\n",
    "\n",
    "def make_outfile_path(organism: str, taxid: str, accver: str) -> Path:\n",
    "    org_clean = _clean_for_filename(_clean_text(organism), max_len=80)\n",
    "    taxid_clean = _clean_for_filename(taxid or \"NA\", max_len=32)\n",
    "    acc_clean = _clean_for_filename(accver, max_len=64)\n",
    "    return outdir / f\"{org_clean}__taxid{taxid_clean}__{acc_clean}.fna\"\n",
    "\n",
    "def make_simple_header(organism: str, wp: str, accver: str, seg_start: int, seg_end: int, taxid: str) -> str:\n",
    "    org_clean = _clean_for_filename(_clean_text(organism), max_len=120) or \"Unknown_organism\"\n",
    "    taxid_txt = (taxid or \"NA\").strip()\n",
    "    return f\">{org_clean}|{wp}|{accver}|{seg_start}-{seg_end}|taxid={taxid_txt}\"\n",
    "\n",
    "def process_one_item(item: CdsMapping) -> tuple[int, int]:\n",
    "    wp = item.wp\n",
    "    accver = item.accver\n",
    "    taxid = (item.taxid or \"\").strip()\n",
    "\n",
    "    seg_start = max(1, min(item.start, item.stop) - int(FLANK))\n",
    "    seg_end   = max(item.start, item.stop) + int(FLANK)\n",
    "\n",
    "    try:\n",
    "        record = fetch_nuccore_fasta_segment(accver, seg_start, seg_end)\n",
    "\n",
    "        with _accver_meta_lock:\n",
    "            cached = _accver_meta_cache.get(accver)\n",
    "\n",
    "        if cached:\n",
    "            organism, cached_taxid = cached\n",
    "            use_taxid = taxid or cached_taxid or \"\"\n",
    "        else:\n",
    "            organism = \"\"\n",
    "            if DO_ORG_LOOKUP:\n",
    "                organism = _get_organism_from_gb(accver, seg_start, seg_end)\n",
    "            if not organism:\n",
    "                organism = item.org or _fallback_org_from_fasta_desc(record) or \"Unknown organism\"\n",
    "            use_taxid = taxid or \"\"\n",
    "            with _accver_meta_lock:\n",
    "                _accver_meta_cache[accver] = (organism, use_taxid)\n",
    "\n",
    "        outpath = make_outfile_path(organism, use_taxid, accver)\n",
    "        header = make_simple_header(organism, wp, accver, seg_start, seg_end, use_taxid)\n",
    "        fasta_text = f\"{header}\\n{record.seq}\\n\"\n",
    "        append_record_to_file(outpath, fasta_text)\n",
    "        return (1, 0)\n",
    "    except Exception:\n",
    "        return (0, 1)\n",
    "\n",
    "# -------------------- Step A: read BLAST in order -> map to taxid -> dedup by taxid -> take first N unique taxids --------------------\n",
    "if not Path(blast_tsv).exists():\n",
    "    raise FileNotFoundError(f\"BLAST TSV not found: {blast_tsv}\")\n",
    "\n",
    "print(f\"[CONFIG] blast_tsv        = {blast_tsv}\")\n",
    "print(f\"[CONFIG] FLANK(bp)        = {FLANK}  (segment = CDS ± FLANK)\")\n",
    "print(f\"[CONFIG] THREADS          = {THREADS}\")\n",
    "print(f\"[CONFIG] TOP_N_UNIQ_TAXID = {TOP_N_UNIQ_TAXID}  (0=all)\")\n",
    "print(f\"[CONFIG] NCBI_EMAIL       = {NCBI_EMAIL}\")\n",
    "print(f\"[CONFIG] API_KEY          = {'set' if NCBI_API_KEY else 'empty'}  (rps={rps})\")\n",
    "print(f\"[CONFIG] PREFER_REFSEQ    = {PREFER_REFSEQ}\")\n",
    "\n",
    "seen_wp = set()\n",
    "seen_taxid = set()\n",
    "picked: list[CdsMapping] = []\n",
    "picked_wp_list: list[str] = []\n",
    "\n",
    "total_lines = 0\n",
    "valid_hit_rows = 0\n",
    "wp_with_no_ipg = 0\n",
    "wp_ipg_error = 0\n",
    "\n",
    "t_pick0 = time.time()\n",
    "\n",
    "with open(blast_tsv, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    for ln in f:\n",
    "        total_lines += 1\n",
    "        ln = ln.rstrip(\"\\n\")\n",
    "        if not ln or ln.startswith(\"#\"):\n",
    "            continue\n",
    "        cols = ln.split(\"\\t\")\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "\n",
    "        valid_hit_rows += 1\n",
    "        wp = normalize_wp(cols[1])\n",
    "        if not wp or wp in seen_wp:\n",
    "            continue\n",
    "        seen_wp.add(wp)\n",
    "\n",
    "        try:\n",
    "            mappings = fetch_ipg_mappings_for_wp(wp)\n",
    "        except Exception:\n",
    "            wp_ipg_error += 1\n",
    "            continue\n",
    "\n",
    "        rep = choose_representative_mapping(mappings)\n",
    "        if rep is None:\n",
    "            wp_with_no_ipg += 1\n",
    "            continue\n",
    "\n",
    "        taxid = (rep.taxid or \"\").strip()\n",
    "        taxid_key = taxid if taxid else f\"NA::{wp}\"\n",
    "\n",
    "        if taxid_key in seen_taxid:\n",
    "            continue\n",
    "\n",
    "        seen_taxid.add(taxid_key)\n",
    "        picked.append(rep)\n",
    "        picked_wp_list.append(wp)\n",
    "\n",
    "        if TOP_N_UNIQ_TAXID and len(picked) >= TOP_N_UNIQ_TAXID:\n",
    "            break\n",
    "\n",
    "elapsed_pick = time.time() - t_pick0\n",
    "\n",
    "wp_list_txt.write_text(\"\\n\".join(picked_wp_list) + (\"\\n\" if picked_wp_list else \"\"), encoding=\"utf-8\")\n",
    "\n",
    "picked_list_txt.write_text(\n",
    "    \"wp\\taccver\\tstart\\tstop\\tstrand\\ttaxid\\torg\\n\" +\n",
    "    \"\\n\".join(\n",
    "        f\"{x.wp}\\t{x.accver}\\t{x.start}\\t{x.stop}\\t{x.strand}\\t{(x.taxid or '').strip()}\\t{(x.org or '').strip()}\"\n",
    "        for x in picked\n",
    "    ) + (\"\\n\" if picked else \"\"),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"[BLAST] total_lines={total_lines}, valid_hit_rows={valid_hit_rows}\")\n",
    "print(f\"[PICK]  unique_taxid_picked={len(picked)} (scan stop={'YES' if (TOP_N_UNIQ_TAXID and len(picked)>=TOP_N_UNIQ_TAXID) else 'NO'})\")\n",
    "print(f\"[PICK]  wp seen={len(seen_wp)}; ipg_error={wp_ipg_error}; no_ipg={wp_with_no_ipg}; pick_elapsed={elapsed_pick:.1f}s\")\n",
    "print(f\"[OUT]   wp list -> {wp_list_txt}\")\n",
    "print(f\"[OUT]   picked items -> {picked_list_txt}\")\n",
    "\n",
    "if not picked:\n",
    "    raise RuntimeError(\"No records were selected. Possible causes: failed to parse WP IDs from the BLAST 2nd column, or IPG queries failed / returned no mapping.\")\n",
    "\n",
    "# -------------------- Step B: threaded download for picked unique-taxid representatives --------------------\n",
    "t0 = time.time()\n",
    "total_succ, total_fail = 0, 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=THREADS) as pool:\n",
    "    futures = [pool.submit(process_one_item, it) for it in picked]\n",
    "    for fu in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading unique taxids\", unit=\"item\"):\n",
    "        s, f_ = fu.result()\n",
    "        total_succ += s\n",
    "        total_fail += f_\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nWrote {total_succ} neighborhoods; failed: {total_fail}\")\n",
    "print(f\"Output directory：{outdir.resolve()}\")\n",
    "print(f\"⏱️ download elapsed: {elapsed:.1f}s\")\n",
    "print(\"FASTA header format: >Organism_with_underscores|WP_xxx|ACCVER|start-end|taxid=xxx\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "c78da213",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "25ce03d0-8b4f-4e1c-f900-f9873160c927"
   },
   "source": [
    "#@title 5) antiSMASH (micromamba): run antiSMASH on downloaded neighborhoods (micromamba, fixed MPLBACKEND, STREAM OUTPUT, prereqs OK; use blast not ncbi-blast)\n",
    "import os, re, hashlib, subprocess, threading, shlex\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Avoid matplotlib_inline backend issues inside antiSMASH\n",
    "os.environ[\"MPLBACKEND\"] = \"agg\"\n",
    "\n",
    "INPUT_DIR=\"wp_flanks_fna\"    #@param {type:\"string\"}\n",
    "OUTROOT=\"antismash_out\"      #@param {type:\"string\"}\n",
    "MAX_JOBS=5                   #@param {type:\"integer\"}\n",
    "CPUS_PER_JOB=1               #@param {type:\"integer\"}\n",
    "DOWNLOAD_DB=True             #@param {type:\"boolean\"}\n",
    "\n",
    "# Stream full antiSMASH stdout/stderr to screen (will interleave when MAX_JOBS>1)\n",
    "STREAM_ANTISMASH=False       #@param {type:\"boolean\"}\n",
    "\n",
    "MM = Path(\"/content/bin/micromamba\")\n",
    "ROOT = Path(\"/content/mm\")\n",
    "ENV = ROOT / \"envs\" / \"antismash8\"\n",
    "DBDIR = Path(\"/content/antismash_db\")\n",
    "\n",
    "# Make micromamba consistent\n",
    "os.environ[\"MAMBA_ROOT_PREFIX\"] = str(ROOT)\n",
    "\n",
    "_print_lock = threading.Lock()\n",
    "def _p(msg: str):\n",
    "    with _print_lock:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "def run_bash(cmd: str, name=\"bash\", stream=True):\n",
    "    \"\"\"Run bash; optionally stream output to notebook; raise with last 200 lines on failure.\"\"\"\n",
    "    cmd = (\n",
    "        \"export MPLBACKEND=agg\\n\"\n",
    "        f\"export MAMBA_ROOT_PREFIX={shlex.quote(str(ROOT))}\\n\"\n",
    "        + cmd\n",
    "    )\n",
    "\n",
    "    if not stream:\n",
    "        p = subprocess.run([\"bash\", \"-lc\", cmd], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        if p.returncode != 0:\n",
    "            tail = \"\\n\".join((p.stdout or \"\").splitlines()[-200:])\n",
    "            raise RuntimeError(f\"[{name}] failed (exit={p.returncode}). Last 200 lines:\\n{tail}\")\n",
    "        return p.stdout\n",
    "\n",
    "    p = subprocess.Popen([\"bash\", \"-lc\", cmd], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1)\n",
    "    last = []\n",
    "    assert p.stdout is not None\n",
    "    for line in p.stdout:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        _p(f\"[{name}] {line}\")\n",
    "        last.append(line)\n",
    "        if len(last) > 200:\n",
    "            last = last[-200:]\n",
    "    rc = p.wait()\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"[{name}] failed (exit={rc}). Last 200 lines:\\n\" + \"\\n\".join(last))\n",
    "    return \"\\n\".join(last)\n",
    "\n",
    "def safe_name(p: Path) -> str:\n",
    "    s = re.sub(r\"[^\\w.\\-]+\", \"_\", p.stem)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    if len(s) <= 140:\n",
    "        return s\n",
    "    h = hashlib.md5(str(p).encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return s[:120] + \"_\" + h\n",
    "\n",
    "# ------------------ 1) Install micromamba ------------------\n",
    "_p(\"[1/4] Installing micromamba (if needed)...\")\n",
    "run_bash(r\"\"\"\n",
    "set -eo pipefail\n",
    "mkdir -p /content/bin\n",
    "if [ ! -x /content/bin/micromamba ]; then\n",
    "  curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj -C /content/bin --strip-components=1 bin/micromamba\n",
    "  chmod +x /content/bin/micromamba\n",
    "fi\n",
    "/content/bin/micromamba --version\n",
    "\"\"\", \"micromamba\", stream=True)\n",
    "\n",
    "# ------------------ 2) Create env + install antismash + prerequisites ------------------\n",
    "_p(\"[2/4] Creating env antismash8 and installing antiSMASH 8 + prerequisites (if needed)...\")\n",
    "run_bash(fr\"\"\"\n",
    "set -eo pipefail\n",
    "mkdir -p \"{ROOT}\"\n",
    "\n",
    "# (A) create base env first (easier to solve)\n",
    "if [ ! -x \"{ENV}/bin/antismash\" ]; then\n",
    "  \"{MM}\" create -y -p \"{ENV}\" -c conda-forge -c bioconda python=3.11 antismash=8\n",
    "fi\n",
    "\n",
    "# (B) then ensure external executables exist\n",
    "# NOTE: use 'blast' (Bioconda) not 'ncbi-blast'\n",
    "\"{MM}\" install -y -p \"{ENV}\" -c conda-forge -c bioconda \\\n",
    "  hmmer hmmer2 diamond blast fasttree prodigal\n",
    "\n",
    "# Show versions + confirm tools are on PATH inside micromamba run\n",
    "\"{MM}\" run -p \"{ENV}\" antismash --version\n",
    "\n",
    "\"{MM}\" run -p \"{ENV}\" bash -lc '\n",
    "set -e\n",
    "for x in hmmscan hmmsearch hmmpress hmmpfam2 diamond blastp makeblastdb fasttree prodigal; do\n",
    "  printf \"%-12s -> \" \"$x\"\n",
    "  command -v \"$x\"\n",
    "done\n",
    "'\n",
    "\"\"\", \"install_antismash\", stream=True)\n",
    "\n",
    "# ------------------ 3) Optional download DB + fix default mite path ------------------\n",
    "_p(\"[3/4] Downloading databases (optional)...\")\n",
    "if DOWNLOAD_DB:\n",
    "    run_bash(fr\"\"\"\n",
    "set -eo pipefail\n",
    "mkdir -p \"{DBDIR}\"\n",
    "\n",
    "# download databases to {DBDIR}\n",
    "HELP=\"$(\"{MM}\" run -p \"{ENV}\" download-antismash-databases --help 2>&1 || true)\"\n",
    "if echo \"$HELP\" | grep -q -- \"--database-dir\"; then\n",
    "  \"{MM}\" run -p \"{ENV}\" download-antismash-databases --database-dir \"{DBDIR}\"\n",
    "elif echo \"$HELP\" | grep -q -- \"--install-dir\"; then\n",
    "  \"{MM}\" run -p \"{ENV}\" download-antismash-databases --install-dir \"{DBDIR}\"\n",
    "else\n",
    "  \"{MM}\" run -p \"{ENV}\" download-antismash-databases \"{DBDIR}\" || \"{MM}\" run -p \"{ENV}\" download-antismash-databases\n",
    "fi\n",
    "\n",
    "# SAFETY: some modules look in package default databases path (not your {DBDIR})\n",
    "AS_DIR=\"$(\"{MM}\" run -p \"{ENV}\" python - <<'PY'\n",
    "import antismash, pathlib\n",
    "print(pathlib.Path(antismash.__file__).resolve().parent)\n",
    "PY\n",
    ")\"\n",
    "PKG_DB=\"$AS_DIR/databases\"\n",
    "MITE_FASTA=\"$(find \"{DBDIR}\" -type f -name 'mite.fasta' -print -quit || true)\"\n",
    "\n",
    "if [ -n \"$MITE_FASTA\" ]; then\n",
    "  MITE_VER_DIR=\"$(dirname \"$MITE_FASTA\")\"\n",
    "  MITE_ROOT=\"$(dirname \"$MITE_VER_DIR\")\"\n",
    "  mkdir -p \"$PKG_DB\"\n",
    "  rm -rf \"$PKG_DB/mite\"\n",
    "  ln -s \"$MITE_ROOT\" \"$PKG_DB/mite\"\n",
    "  echo \"[FIX] linked $PKG_DB/mite -> $MITE_ROOT\"\n",
    "else\n",
    "  echo \"[WARN] mite.fasta not found under {DBDIR} (genefunctions/MITE may fail).\"\n",
    "fi\n",
    "\n",
    "# IMPORTANT: do a REAL prereq check (do not swallow failures)\n",
    "\"{MM}\" run -p \"{ENV}\" antismash --check-prereqs --databases \"{DBDIR}\"\n",
    "\"\"\", \"download_db\", stream=True)\n",
    "else:\n",
    "    _p(\"  - DOWNLOAD_DB=False, skipped.\")\n",
    "\n",
    "# ------------------ 4) Run batch ------------------\n",
    "_p(\"[4/4] Running antiSMASH batch...\")\n",
    "\n",
    "indir = Path(INPUT_DIR)\n",
    "outroot = Path(OUTROOT)\n",
    "logs = Path(\"logs\")\n",
    "outroot.mkdir(parents=True, exist_ok=True)\n",
    "logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = sorted(indir.glob(\"*.fna\"))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No .fna files found under: {indir.resolve()}\")\n",
    "\n",
    "# Detect zip flag via micromamba run (ensures PATH is correct)\n",
    "help_text = subprocess.check_output(\n",
    "    [str(MM), \"run\", \"-p\", str(ENV), \"antismash\", \"--help\"],\n",
    "    text=True,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env={**os.environ, \"MPLBACKEND\": \"agg\", \"MAMBA_ROOT_PREFIX\": str(ROOT)},\n",
    ")\n",
    "zip_flag = \"--no-zip-output\" if \"--no-zip-output\" in help_text else \"\"\n",
    "\n",
    "BASE_ARGS = [a for a in [\n",
    "    zip_flag,\n",
    "    \"--taxon\", \"bacteria\",\n",
    "    \"--genefinding-tool\", \"prodigal\",\n",
    "    \"--allow-long-headers\",\n",
    "] if a]\n",
    "\n",
    "DB_ARGS = []\n",
    "if DOWNLOAD_DB and DBDIR.exists():\n",
    "    DB_ARGS += [\"--databases\", str(DBDIR)]\n",
    "    DB_ARGS += [\n",
    "        \"--tfbs\", \"--cb-general\", \"--cb-knownclusters\", \"--cb-subclusters\",\n",
    "        \"--fullhmmer\", \"--rre\", \"--cc-mibig\", \"--tigrfam\", \"--asf\", \"--pfam2go\",\n",
    "        \"--smcog-trees\", \"--clusterhmmer\",\n",
    "    ]\n",
    "else:\n",
    "    if DOWNLOAD_DB:\n",
    "        _p(f\"[WARN] DOWNLOAD_DB=True but {DBDIR} not found; running without DB-dependent modules.\")\n",
    "\n",
    "COMMON_ARGS = [*BASE_ARGS, *DB_ARGS]\n",
    "\n",
    "def run_one(f: Path):\n",
    "    name = safe_name(f)\n",
    "    outdir = outroot / name\n",
    "    idx = outdir / \"index.html\"\n",
    "    if idx.exists():\n",
    "        _p(f\"[SKIP] {f.name} (already has index.html)\")\n",
    "        return (\"skipped\", str(f))\n",
    "\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # IMPORTANT: run via micromamba so PATH includes ENV/bin\n",
    "    cmd = [\n",
    "        str(MM), \"run\", \"-p\", str(ENV),\n",
    "        \"antismash\",\n",
    "        *COMMON_ARGS,\n",
    "        \"-c\", str(int(CPUS_PER_JOB)),\n",
    "        \"--output-dir\", str(outdir),\n",
    "        str(f),\n",
    "    ]\n",
    "\n",
    "    env = {**os.environ, \"MPLBACKEND\": \"agg\", \"MAMBA_ROOT_PREFIX\": str(ROOT)}\n",
    "\n",
    "    _p(f\"[RUN ] {f.name} -> {outdir}\")\n",
    "\n",
    "    if STREAM_ANTISMASH:\n",
    "        p = subprocess.Popen(cmd, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, env=env)\n",
    "        last = []\n",
    "        assert p.stdout is not None\n",
    "        for line in p.stdout:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            _p(f\"[{f.name}] {line}\")\n",
    "            last.append(line)\n",
    "            if len(last) > 300:\n",
    "                last = last[-300:]\n",
    "        rc = p.wait()\n",
    "        out = \"\\n\".join(last)  # store tail only\n",
    "        err = \"\"\n",
    "    else:\n",
    "        p = subprocess.run(cmd, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n",
    "        rc, out, err = p.returncode, (p.stdout or \"\"), (p.stderr or \"\")\n",
    "\n",
    "    (logs / f\"{name}.stdout.log\").write_text(out, encoding=\"utf-8\")\n",
    "    (logs / f\"{name}.stderr.log\").write_text(err, encoding=\"utf-8\")\n",
    "\n",
    "    if rc != 0 or not idx.exists():\n",
    "        _p(f\"[FAIL] {f.name} (exit={rc})\")\n",
    "        return (\"failed\", str(f))\n",
    "\n",
    "    _p(f\"[ OK ] {f.name}\")\n",
    "    return (\"ok\", str(f))\n",
    "\n",
    "ok = failed = skipped = 0\n",
    "failed_paths = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=int(MAX_JOBS)) as pool:\n",
    "    futs = [pool.submit(run_one, f) for f in files]\n",
    "    for fu in as_completed(futs):\n",
    "        status, fn = fu.result()\n",
    "        if status == \"ok\":\n",
    "            ok += 1\n",
    "        elif status == \"failed\":\n",
    "            failed += 1\n",
    "            failed_paths.append(fn)\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "failed_list = logs / \"failed_files.txt\"\n",
    "failed_list.write_text(\"\\n\".join(failed_paths) + (\"\\n\" if failed_paths else \"\"), encoding=\"utf-8\")\n",
    "\n",
    "_p(f\"\\nDone. ok={ok}, failed={failed}, skipped={skipped}\")\n",
    "_p(f\"OUTROOT: {outroot.resolve()}\")\n",
    "_p(f\"Logs: {logs.resolve()}\")\n",
    "_p(f\"Failed list: {failed_list.resolve()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "b162a4fb"
   },
   "source": [
    "#@title 6) cblaster: upload query FASTA, build local DB from antiSMASH region GBKs, and run a local search\n",
    "#@markdown This cell installs cblaster (via micromamba), builds a Diamond database from antiSMASH region GenBank files, and runs cblaster search.\n",
    "\n",
    "\n",
    "# ---- user-configurable params ----\n",
    "U  = 3      #@param {type:\"integer\"}\n",
    "MH = 3      #@param {type:\"integer\"}\n",
    "MI = 20     #@param {type:\"integer\"}\n",
    "MC = 50     #@param {type:\"integer\"}\n",
    "HS = 10000  #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "# ---- fixed paths/settings ----\n",
    "import os\n",
    "NCBI_EMAIL = os.environ.get('NCBI_EMAIL', globals().get('EMAIL','')).strip()\n",
    "NCBI_API_KEY = os.environ.get('NCBI_API_KEY','').strip()\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, shlex, subprocess, threading\n",
    "\n",
    "MAMBA_ROOT = Path(\"/content/mm\")\n",
    "MM_BIN     = Path(\"/content/bin/micromamba\")\n",
    "ENV        = MAMBA_ROOT / \"envs\" / \"antismash8\"\n",
    "\n",
    "ANTISMASH_OUTROOT = Path(\"antismash_out\")\n",
    "REGION_PATTERNS   = (\"*region*.gbk\", \"*region*.gbff\", \"*region*.gb\")\n",
    "\n",
    "DB_DIR  = Path(\"/content/pfaA-cblasterdb\")\n",
    "DB_NAME = \"pfaa\"\n",
    "\n",
    "OUTPUT_DECIMALS  = 2\n",
    "BINARY_DELIMITER = \",\"\n",
    "STREAM_OUTPUT    = True\n",
    "\n",
    "# ---- validate (same style as your WP cell) ----\n",
    "NCBI_EMAIL = (NCBI_EMAIL or \"\").strip()\n",
    "NCBI_API_KEY = (NCBI_API_KEY or \"\").strip()\n",
    "\n",
    "if not NCBI_EMAIL:\n",
    "    raise ValueError(\"NCBI_EMAIL is required for cblaster config / Entrez settings.\")\n",
    "\n",
    "if U < 1 or MH < 1 or MI < 0 or MC < 0 or HS < 0:\n",
    "    raise ValueError(\"Invalid parameter ranges: u/mh>=1; mi/mc/hs>=0.\")\n",
    "\n",
    "# ---- helpers ----\n",
    "_print_lock = threading.Lock()\n",
    "def _p(msg: str):\n",
    "    with _print_lock:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "def run_bash(cmd: str, name=\"bash\", stream=True, check=True):\n",
    "    cmd = f\"export MAMBA_ROOT_PREFIX={shlex.quote(str(MAMBA_ROOT))}\\n\" + cmd\n",
    "    if not stream:\n",
    "        p = subprocess.run([\"bash\", \"-lc\", cmd], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        out = p.stdout or \"\"\n",
    "        if check and p.returncode != 0:\n",
    "            tail = \"\\n\".join(out.splitlines()[-200:])\n",
    "            raise RuntimeError(f\"[{name}] failed (exit={p.returncode}). Last 200 lines:\\n{tail}\")\n",
    "        return out\n",
    "\n",
    "    p = subprocess.Popen([\"bash\", \"-lc\", cmd], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1)\n",
    "    last = []\n",
    "    assert p.stdout is not None\n",
    "    for line in p.stdout:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        _p(f\"[{name}] {line}\")\n",
    "        last.append(line)\n",
    "        if len(last) > 200:\n",
    "            last = last[-200:]\n",
    "    rc = p.wait()\n",
    "    if check and rc != 0:\n",
    "        raise RuntimeError(f\"[{name}] failed (exit={rc}). Last 200 lines:\\n\" + \"\\n\".join(last))\n",
    "    return \"\\n\".join(last)\n",
    "\n",
    "def mm_run(args, name=\"mm\", stream=True, check=True):\n",
    "    if not MM_BIN.exists():\n",
    "        raise FileNotFoundError(f\"micromamba not found: {MM_BIN}. Run the antiSMASH/micromamba setup cell first.\")\n",
    "    if not ENV.exists():\n",
    "        raise FileNotFoundError(f\"Environment not found: {ENV}. Run the antiSMASH environment creation cell first.\")\n",
    "\n",
    "    cmd = [str(MM_BIN), \"run\", \"-p\", str(ENV), *args]\n",
    "    env = {**os.environ, \"MAMBA_ROOT_PREFIX\": str(MAMBA_ROOT)}\n",
    "\n",
    "    if not stream:\n",
    "        p = subprocess.run(cmd, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env)\n",
    "        out = p.stdout or \"\"\n",
    "        if check and p.returncode != 0:\n",
    "            tail = \"\\n\".join(out.splitlines()[-200:])\n",
    "            raise RuntimeError(f\"[{name}] failed (exit={p.returncode}). Last 200 lines:\\n{tail}\")\n",
    "        return out\n",
    "\n",
    "    p = subprocess.Popen(cmd, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, env=env)\n",
    "    last = []\n",
    "    assert p.stdout is not None\n",
    "    for line in p.stdout:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        _p(f\"[{name}] {line}\")\n",
    "        last.append(line)\n",
    "        if len(last) > 300:\n",
    "            last = last[-300:]\n",
    "    rc = p.wait()\n",
    "    if check and rc != 0:\n",
    "        raise RuntimeError(f\"[{name}] failed (exit={rc}). Last 300 lines:\\n\" + \"\\n\".join(last))\n",
    "    return \"\\n\".join(last)\n",
    "\n",
    "def sanitize_prefix(s: str, fallback=\"query\"):\n",
    "    s = re.sub(r\"[^\\w.\\-]+\", \"_\", (s or \"\").strip())\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s[:80] if s else fallback\n",
    "\n",
    "def ensure_query_fasta(upload_path: Path, out_fa: Path) -> str:\n",
    "    txt = upload_path.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "    if not txt:\n",
    "        raise ValueError(f\"Uploaded file is empty: {upload_path}\")\n",
    "\n",
    "    if txt.lstrip().startswith(\">\"):\n",
    "        lines = [ln.rstrip() for ln in txt.splitlines() if ln.strip() != \"\"]\n",
    "        header = lines[0][1:].strip()\n",
    "        prefix = sanitize_prefix(header.split()[0] if header else out_fa.stem)\n",
    "        out_fa.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "        return prefix\n",
    "\n",
    "    seq = re.sub(r\"\\s+\", \"\", txt)\n",
    "    if re.search(r\"[^A-Za-z\\*\\-\\.]\", seq):\n",
    "        bad = sorted(set(re.findall(r\"[^A-Za-z\\*\\-\\.]\", seq)))\n",
    "        raise ValueError(f\"Uploaded sequence contains unsupported characters: {bad}\")\n",
    "    prefix = sanitize_prefix(upload_path.stem, \"query\")\n",
    "    out_fa.write_text(f\">{prefix}\\n{seq}\\n\", encoding=\"utf-8\")\n",
    "    return prefix\n",
    "\n",
    "def collect_region_files(root: Path):\n",
    "    region_files = []\n",
    "    for pat in REGION_PATTERNS:\n",
    "        region_files.extend([p for p in root.rglob(pat) if p.is_file()])\n",
    "    return sorted(set(region_files))\n",
    "\n",
    "def ensure_cblaster_installed():\n",
    "    try:\n",
    "        mm_run([\"cblaster\", \"--version\"], name=\"cblaster_version\", stream=False, check=True)\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    run_bash(fr\"\"\"\n",
    "set -euo pipefail\n",
    "\"{MM_BIN}\" install -y -p \"{ENV}\" -c conda-forge -c bioconda cblaster diamond blast\n",
    "\"{MM_BIN}\" run -p \"{ENV}\" cblaster --version\n",
    "\"{MM_BIN}\" run -p \"{ENV}\" diamond version || true\n",
    "\"{MM_BIN}\" run -p \"{ENV}\" bash -lc 'blastp -version | head -n 1' || true\n",
    "\"\"\", name=\"install_cblaster\", stream=STREAM_OUTPUT, check=True)\n",
    "\n",
    "def ensure_cblaster_config(email: str, api_key: str):\n",
    "    help_txt = mm_run([\"cblaster\", \"config\", \"-h\"], name=\"cblaster_config_help\", stream=False, check=False)\n",
    "    email_flag = \"--email\" if \"--email\" in help_txt else None\n",
    "    apikey_flag = \"--api_key\" if \"--api_key\" in help_txt else (\"--api-key\" if \"--api-key\" in help_txt else None)\n",
    "    force_flag = \"--force\" if \"--force\" in help_txt else (\"-f\" if re.search(r\"(?m)^\\s*-f[, ]\", help_txt) else None)\n",
    "\n",
    "    if not email_flag:\n",
    "        raise RuntimeError(\"Could not detect the --email option from `cblaster config -h` output.\")\n",
    "\n",
    "    cmd = [\"cblaster\", \"config\"]\n",
    "    if force_flag:\n",
    "        cmd += [force_flag]\n",
    "    cmd += [email_flag, email]\n",
    "    if api_key and apikey_flag:\n",
    "        cmd += [apikey_flag, api_key]\n",
    "\n",
    "    mm_run(cmd, name=\"cblaster_config\", stream=STREAM_OUTPUT, check=True)\n",
    "\n",
    "def build_db_if_needed(region_files):\n",
    "    DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    dmnd = DB_DIR / f\"{DB_NAME}.dmnd\"\n",
    "    sqlite3 = DB_DIR / f\"{DB_NAME}.sqlite3\"\n",
    "    fasta = DB_DIR / f\"{DB_NAME}.fasta\"\n",
    "\n",
    "    if dmnd.exists() and sqlite3.exists() and fasta.exists():\n",
    "        _p(f\"[INFO] DB exists; reuse: {dmnd}\")\n",
    "        return dmnd, sqlite3, fasta\n",
    "\n",
    "    _p(f\"[INFO] region files found: {len(region_files)}\")\n",
    "    _p(f\"[INFO] building DB: {DB_DIR}/{DB_NAME}.*\")\n",
    "\n",
    "    stage = DB_DIR / \"_stage_gbk\"\n",
    "    stage.mkdir(parents=True, exist_ok=True)\n",
    "    for x in stage.glob(\"*\"):\n",
    "        try:\n",
    "            x.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for i, p in enumerate(region_files, 1):\n",
    "        suf = p.suffix if p.suffix else \".gbk\"\n",
    "        (stage / f\"r{i:06d}{suf}\").symlink_to(p)\n",
    "\n",
    "    cmd = f'cd {shlex.quote(str(DB_DIR))} && cblaster makedb -n {shlex.quote(DB_NAME)} _stage_gbk/*'\n",
    "    mm_run([\"bash\", \"-lc\", cmd], name=\"cblaster_makedb\", stream=STREAM_OUTPUT, check=True)\n",
    "\n",
    "    if not (dmnd.exists() and sqlite3.exists() and fasta.exists()):\n",
    "        raise RuntimeError(f\"DB build finished but outputs missing in {DB_DIR}: {dmnd.name}, {sqlite3.name}, {fasta.name}\")\n",
    "\n",
    "    return dmnd, sqlite3, fasta\n",
    "\n",
    "def run_local_search(query_fa: Path, dmnd: Path, prefix: str):\n",
    "    plot_html    = Path(f\"my_plot-{prefix}.html\").resolve()\n",
    "    session_json = Path(f\"session-{prefix}.json\").resolve()\n",
    "    summary_csv  = Path(f\"summary-{prefix}.csv\").resolve()\n",
    "    abspres_csv  = Path(f\"abspres-{prefix}.csv\").resolve()\n",
    "\n",
    "    args = [\n",
    "        \"cblaster\", \"search\",\n",
    "        \"-qf\", str(query_fa),\n",
    "        \"--plot\", str(plot_html),\n",
    "        \"--session_file\", str(session_json),\n",
    "        \"--output\", str(summary_csv),\n",
    "        \"--output_decimals\", str(int(OUTPUT_DECIMALS)),\n",
    "        \"--binary\", str(abspres_csv),\n",
    "        \"--binary_delimiter\", str(BINARY_DELIMITER),\n",
    "        \"-u\", str(int(U)),\n",
    "        \"-mh\", str(int(MH)),\n",
    "        \"-mi\", str(int(MI)),\n",
    "        \"-mc\", str(int(MC)),\n",
    "        \"-hs\", str(int(HS)),\n",
    "        \"-db\", str(dmnd),\n",
    "        \"--mode\", \"local\",\n",
    "    ]\n",
    "\n",
    "    _p(\"[RUN] \" + \" \".join(shlex.quote(x) for x in args))\n",
    "    mm_run(args, name=\"cblaster_search\", stream=STREAM_OUTPUT, check=True)\n",
    "\n",
    "    _p(\"\\n Done.\")\n",
    "    _p(f\"📄 Plot:    {plot_html}\")\n",
    "    _p(f\" Session: {session_json}\")\n",
    "    _p(f\"📑 Summary: {summary_csv}\")\n",
    "    _p(f\"📊 Binary:  {abspres_csv}\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "print(f\"[CONFIG] NCBI_EMAIL = {NCBI_EMAIL}\")\n",
    "print(f\"[CONFIG] API_KEY    = {'set' if NCBI_API_KEY else 'empty'}\")\n",
    "print(f\"[CONFIG] -u {U}  -mh {MH}  -mi {MI}  -mc {MC}  -hs {HS}\")\n",
    "\n",
    "# Step 1: upload query\n",
    "_p(\"[1/4] Upload query FASTA (or AA .txt)...\")\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Colab upload not available: {e}\")\n",
    "if not uploaded:\n",
    "    raise RuntimeError(\"No file uploaded.\")\n",
    "\n",
    "up_name = next(iter(uploaded.keys()))\n",
    "up_path = Path(\"/content\") / up_name\n",
    "query_fa = Path(\"/content/cblaster_query.faa\")\n",
    "prefix = ensure_query_fasta(up_path, query_fa)\n",
    "prefix = sanitize_prefix(prefix, \"query\")\n",
    "_p(f\"[INFO] query = {query_fa}  prefix={prefix}\")\n",
    "\n",
    "# Step 2: ensure tool\n",
    "_p(\"[2/4] Ensure cblaster installed...\")\n",
    "ensure_cblaster_installed()\n",
    "\n",
    "# Step 3: ensure config (this fixes your previous error)\n",
    "_p(\"[3/4] Run cblaster config (required by cblaster 1.4.x)...\")\n",
    "ensure_cblaster_config(NCBI_EMAIL, NCBI_API_KEY)\n",
    "\n",
    "# Step 4: build DB + search\n",
    "_p(\"[4/4] Build/validate local DB from antiSMASH region GBKs, then search...\")\n",
    "root = ANTISMASH_OUTROOT.resolve()\n",
    "if not root.exists():\n",
    "    raise FileNotFoundError(f\"antiSMASH output root not found: {root}\")\n",
    "\n",
    "region_files = collect_region_files(root)\n",
    "if not region_files:\n",
    "    raise FileNotFoundError(f\"No region GenBanks found under {root} matching {REGION_PATTERNS}\")\n",
    "\n",
    "dmnd, sqlite3, fasta = build_db_if_needed(region_files)\n",
    "run_local_search(query_fa, dmnd, prefix)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "a33e81ba"
   },
   "source": [
    "#@title 7) Rule-based filter and copy passing antiSMASH region folders/files\n",
    "#@markdown This cell reads a cblaster abs/presence CSV/TSV (e.g., **abspres-*.csv**) to select antiSMASH region GenBank files and applies a four-step rule-based filter.\n",
    "#@markdown It then copies either the matching GBK files or their parent directories into **DEST**.\n",
    "\n",
    "\n",
    "import os, re, shutil, time\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- user-configurable params ----------------\n",
    "INPUT = \"\"\n",
    "ROOT  = \"\"\n",
    "DEST  = \"final-target\"\n",
    "COPY_MODE = \"dir\"\n",
    "COL = 1\n",
    "PROTO_GAP_MAX = 25000  #@param {type:\"integer\"}\n",
    "OVERWRITE = False\n",
    "DRY_RUN = False\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **k):  # fallback\n",
    "        return x\n",
    "\n",
    "def _p(msg: str):\n",
    "    print(msg, flush=True)\n",
    "\n",
    "def detect_fs(first_line: str):\n",
    "    if \"\\t\" in first_line:\n",
    "        return \"\\t\"\n",
    "    if \",\" in first_line:\n",
    "        return \",\"\n",
    "    return None  # whitespace-split\n",
    "\n",
    "def trim_cell(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.lstrip(\"\\ufeff\")  # BOM\n",
    "    s = s.strip().strip('\"').strip(\"'\").strip()\n",
    "    s = re.sub(r\"\\r$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def normalize_region_base(s: str):\n",
    "    s = trim_cell(s)\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.split(\"#\", 1)[0].strip()\n",
    "    s = re.sub(r\"^.*\\/\", \"\", s)  # drop path\n",
    "    s = re.sub(r\"\\.(gbk|gbff|gb)$\", \"\", s, flags=re.I)\n",
    "    m = re.search(r\"([^\\s,;\\\"']+\\.region\\d+)$\", s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None\n",
    "\n",
    "def extract_region_names(input_path: Path, col: int = 1):\n",
    "    txt = input_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    first = next((ln for ln in txt if ln.strip()), \"\")\n",
    "    if not first:\n",
    "        raise ValueError(f\"Input file is empty: {input_path}\")\n",
    "    fs = detect_fs(first)\n",
    "\n",
    "    names = set()\n",
    "\n",
    "    # ---- by column (preferred) ----\n",
    "    if fs is not None:\n",
    "        for i, ln in enumerate(txt, start=1):\n",
    "            if not ln.strip():\n",
    "                continue\n",
    "            parts = ln.split(fs)\n",
    "            if len(parts) < col:\n",
    "                continue\n",
    "            v = trim_cell(parts[col-1])\n",
    "            # skip header-like first line\n",
    "            if i == 1 and v.lower() in {\"organism\",\"scaffold\",\"name\",\"filename\"}:\n",
    "                continue\n",
    "            b = normalize_region_base(v)\n",
    "            if b:\n",
    "                names.add(b)\n",
    "    else:\n",
    "        # whitespace\n",
    "        for i, ln in enumerate(txt, start=1):\n",
    "            if not ln.strip():\n",
    "                continue\n",
    "            parts = re.split(r\"\\s+\", ln.strip())\n",
    "            if len(parts) < col:\n",
    "                continue\n",
    "            v = trim_cell(parts[col-1])\n",
    "            if i == 1 and v.lower() in {\"organism\",\"scaffold\",\"name\",\"filename\"}:\n",
    "                continue\n",
    "            b = normalize_region_base(v)\n",
    "            if b:\n",
    "                names.add(b)\n",
    "\n",
    "    # ---- fallback regex if empty ----\n",
    "    if not names:\n",
    "        blob = \"\\n\".join(txt)\n",
    "        for m in re.finditer(r\"([^\\s,;\\\"']+\\.region\\d+)\", blob):\n",
    "            b = normalize_region_base(m.group(1))\n",
    "            if b:\n",
    "                names.add(b)\n",
    "\n",
    "    return sorted(names)\n",
    "\n",
    "def index_region_files(root: Path):\n",
    "    exts = {\".gbk\",\".gbff\",\".gb\"}\n",
    "    base2paths = {}\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for fn in filenames:\n",
    "            suf = Path(fn).suffix.lower()\n",
    "            if suf not in exts:\n",
    "                continue\n",
    "            p = Path(dirpath) / fn\n",
    "            base = re.sub(r\"\\.(gbk|gbff|gb)$\", \"\", fn, flags=re.I)\n",
    "            base2paths.setdefault(base, []).append(p)\n",
    "    return base2paths\n",
    "\n",
    "# ---- step3 parsing: protocluster core_location + PFAM_domain de-dup ----\n",
    "_core_re = re.compile(r'/core_location=\"?\\[(\\d+):(\\d+)\\]\\([+-]\\)\"?')\n",
    "_prod_re = re.compile(r'/product=\"([^\"]+)\"')\n",
    "\n",
    "_pfam_re   = re.compile(r'/db_xref=\"(PF\\d+)\\.\\d+\"')\n",
    "_asdom_re  = re.compile(r'/aSDomain=\"([^\"]+)\"')\n",
    "_ps_re     = re.compile(r'/protein_start=\"(\\d+)\"')\n",
    "_pe_re     = re.compile(r'/protein_end=\"(\\d+)\"')\n",
    "\n",
    "def _is_qual(line: str) -> bool:\n",
    "    return line.startswith(\"                     \")  # 21 spaces\n",
    "\n",
    "def parse_protoclusters_core(lines):\n",
    "    clusters = []\n",
    "    i, n = 0, len(lines)\n",
    "    while i < n:\n",
    "        if lines[i].startswith(\"     protocluster\"):\n",
    "            cur = {\"product\": None, \"core_start\": None, \"core_end\": None}\n",
    "            i += 1\n",
    "            while i < n and _is_qual(lines[i]):\n",
    "                s = lines[i].strip()\n",
    "                m = _prod_re.search(s)\n",
    "                if m:\n",
    "                    cur[\"product\"] = m.group(1)\n",
    "                m = _core_re.search(s)\n",
    "                if m:\n",
    "                    a, b = int(m.group(1)), int(m.group(2))\n",
    "                    cur[\"core_start\"], cur[\"core_end\"] = (a, b) if a <= b else (b, a)\n",
    "                i += 1\n",
    "            if cur[\"product\"] and cur[\"core_start\"] is not None and cur[\"core_end\"] is not None:\n",
    "                clusters.append(cur)\n",
    "            continue\n",
    "        i += 1\n",
    "    return clusters\n",
    "\n",
    "def gap_half_open(a_start, a_end, b_start, b_end) -> int:\n",
    "    # treat [start:end] as half-open [start, end)\n",
    "    left = max(a_start, b_start)\n",
    "    right = min(a_end, b_end)\n",
    "    if left < right:\n",
    "        return 0\n",
    "    if a_end <= b_start:\n",
    "        return b_start - a_end\n",
    "    if b_end <= a_start:\n",
    "        return a_start - b_end\n",
    "    return 0\n",
    "\n",
    "def parse_unique_pfam_domains(lines):\n",
    "    \"\"\"\n",
    "    Only PFAM_domain features are parsed; deduplicate by (location, PF set, aSDomain, protein_start, protein_end),\n",
    "    to remove duplicates between fullhmmer and clusterhmmer.\n",
    "    \"\"\"\n",
    "    domains = set()\n",
    "    i, n = 0, len(lines)\n",
    "    while i < n:\n",
    "        if lines[i].startswith(\"     PFAM_domain\"):\n",
    "            loc = lines[i][len(\"     PFAM_domain\"):].strip()\n",
    "            pfams = set()\n",
    "            asdom = \"\"\n",
    "            ps = None\n",
    "            pe = None\n",
    "            i += 1\n",
    "            while i < n and _is_qual(lines[i]):\n",
    "                s = lines[i].strip()\n",
    "                m = _pfam_re.search(s)\n",
    "                if m:\n",
    "                    pfams.add(m.group(1))\n",
    "                m = _asdom_re.search(s)\n",
    "                if m:\n",
    "                    asdom = m.group(1) or \"\"\n",
    "                m = _ps_re.search(s)\n",
    "                if m:\n",
    "                    ps = int(m.group(1))\n",
    "                m = _pe_re.search(s)\n",
    "                if m:\n",
    "                    pe = int(m.group(1))\n",
    "                i += 1\n",
    "            key = (loc, tuple(sorted(pfams)), asdom, ps, pe)\n",
    "            domains.add(key)\n",
    "            continue\n",
    "        i += 1\n",
    "    return domains\n",
    "\n",
    "def count_domains(domains):\n",
    "    pf00501_n = 0\n",
    "    amp_n = 0\n",
    "    cond_n = 0\n",
    "\n",
    "    for loc, pfams, asdom, ps, pe in domains:\n",
    "        pfams = set(pfams)\n",
    "        asdom_l = (asdom or \"\").lower()\n",
    "\n",
    "        if \"PF00501\" in pfams:\n",
    "            pf00501_n += 1\n",
    "        if asdom_l == \"amp-binding\":\n",
    "            amp_n += 1\n",
    "\n",
    "        # Condensation: count ONCE per PFAM_domain feature\n",
    "        is_cond = (\"PF00668\" in pfams) or (asdom_l in {\"condensation_lcl\",\"condensation_dcl\"})\n",
    "        if is_cond:\n",
    "            cond_n += 1\n",
    "\n",
    "    return pf00501_n, amp_n, cond_n\n",
    "\n",
    "def passes_filters(gbk_path: Path) -> bool:\n",
    "    text = gbk_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # step1\n",
    "    if re.search(r\"PKS_AT\", text, flags=re.I) is None:\n",
    "        return False\n",
    "    # step2\n",
    "    if re.search(r\"ketoacyl synthase\", text, flags=re.I) is None:\n",
    "        return False\n",
    "\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # step3b domain counts (PFAM_domain only)\n",
    "    domains = parse_unique_pfam_domains(lines)\n",
    "    pf00501_n, amp_n, cond_n = count_domains(domains)\n",
    "    if not ((pf00501_n >= 2 or amp_n >= 2) and cond_n >= 2):\n",
    "        return False\n",
    "\n",
    "    # step3a protocluster core_location gap\n",
    "    clusters = parse_protoclusters_core(lines)\n",
    "    nrps = [c for c in clusters if (c[\"product\"] or \"\").strip().upper() == \"NRPS\"]\n",
    "    hgl  = [c for c in clusters if \"HGLE-KS\" in (c[\"product\"] or \"\").strip().upper()]\n",
    "    if not nrps or not hgl:\n",
    "        return False\n",
    "\n",
    "    min_gap = None\n",
    "    for a in nrps:\n",
    "        for b in hgl:\n",
    "            g = gap_half_open(a[\"core_start\"], a[\"core_end\"], b[\"core_start\"], b[\"core_end\"])\n",
    "            min_gap = g if min_gap is None else min(min_gap, g)\n",
    "    if min_gap is None or min_gap > int(PROTO_GAP_MAX):\n",
    "        return False\n",
    "\n",
    "    # step4 exact /product=\"NRPS\"\n",
    "    if '/product=\"NRPS\"' not in text:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def write_list(p: Path, items):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(\"\\n\".join(map(str, items)) + (\"\\n\" if items else \"\"), encoding=\"utf-8\")\n",
    "\n",
    "# ---------------- auto-resolve paths ----------------\n",
    "prefix = globals().get(\"prefix\", \"query\")\n",
    "ANTISMASH_OUTROOT = globals().get(\"ANTISMASH_OUTROOT\", Path(\"antismash_out\"))\n",
    "\n",
    "root = Path(ROOT.strip()) if str(ROOT).strip() else Path(ANTISMASH_OUTROOT)\n",
    "root = root.resolve()\n",
    "dest = Path(DEST).resolve()\n",
    "\n",
    "if not INPUT.strip():\n",
    "    cand = [Path(f\"abspres-{prefix}.csv\"), Path(f\"summary-{prefix}.csv\")]\n",
    "    cand = [c for c in cand if c.exists()]\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(\"INPUT is empty and no abspres-*.csv / summary-*.csv was found. Please provide INPUT as a file path.\")\n",
    "    input_path = cand[0].resolve()\n",
    "else:\n",
    "    input_path = Path(INPUT).expanduser().resolve()\n",
    "\n",
    "_p(f\"[INFO] INPUT={input_path}\")\n",
    "_p(f\"[INFO] ROOT ={root}\")\n",
    "_p(f\"[INFO] DEST ={dest}\")\n",
    "_p(f\"[INFO] COPY_MODE={COPY_MODE}  COL={COL}  PROTO_GAP_MAX={PROTO_GAP_MAX}\")\n",
    "_p(f\"[INFO] OVERWRITE={OVERWRITE}  DRY_RUN={DRY_RUN}\")\n",
    "\n",
    "if not input_path.exists():\n",
    "    raise FileNotFoundError(f\"INPUT not found: {input_path}\")\n",
    "if not root.exists():\n",
    "    raise FileNotFoundError(f\"ROOT not found: {root}\")\n",
    "\n",
    "dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# 1) names\n",
    "names = extract_region_names(input_path, col=int(COL))\n",
    "write_list(dest / \"extracted_names.txt\", names)\n",
    "_p(f\"[INFO] extracted names: {len(names)}  -> {dest/'extracted_names.txt'}\")\n",
    "_p(f\"[INFO] first 3 names: {names[:3]}\")\n",
    "\n",
    "# 2) index + match\n",
    "_p(\"[INFO] indexing region files under ROOT ...\")\n",
    "base2paths = index_region_files(root)\n",
    "all_bases = sorted(base2paths.keys())\n",
    "write_list(dest / \"all_region_bases.txt\", all_bases)\n",
    "\n",
    "matched = []\n",
    "for nm in names:\n",
    "    matched.extend(base2paths.get(nm, []))\n",
    "matched = sorted(set(matched))\n",
    "write_list(dest / \"matched_region_files.abs.txt\", [str(p) for p in matched])\n",
    "\n",
    "unmatched = sorted(set(names) - set(base2paths.keys()))\n",
    "write_list(dest / \"unmatched_names.txt\", unmatched)\n",
    "\n",
    "_p(f\"[INFO] matched gbk files: {len(matched)} -> {dest/'matched_region_files.abs.txt'}\")\n",
    "_p(f\"[INFO] unmatched names:   {len(unmatched)} -> {dest/'unmatched_names.txt'}\")\n",
    "\n",
    "if not matched:\n",
    "    _p(\"[WARN] No matched gbk files. Stop.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# 3) filters\n",
    "kept = []\n",
    "_p(\"[INFO] filtering matched gbk files ...\")\n",
    "for p in tqdm(matched, total=len(matched)):\n",
    "    try:\n",
    "        if passes_filters(p):\n",
    "            kept.append(p)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "kept = sorted(set(kept))\n",
    "write_list(dest / \"kept_region_files.abs.txt\", [str(p) for p in kept])\n",
    "\n",
    "_p(f\"[INFO] kept gbk files: {len(kept)} -> {dest/'kept_region_files.abs.txt'}\")\n",
    "\n",
    "if not kept:\n",
    "    _p(\"[INFO] No files passed filters; nothing to copy.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# 4) decide targets\n",
    "if COPY_MODE == \"dir\":\n",
    "    targets = sorted({p.parent for p in kept})\n",
    "else:\n",
    "    targets = kept[:]\n",
    "\n",
    "write_list(dest / \"kept_targets.abs.txt\", [str(p) for p in targets])\n",
    "\n",
    "# 5) copy\n",
    "if DRY_RUN:\n",
    "    _p(\"[DRY_RUN] skip copying.\")\n",
    "else:\n",
    "    _p(\"[INFO] copying targets ...\")\n",
    "    for src in tqdm(targets, total=len(targets)):\n",
    "        src = Path(src)\n",
    "        try:\n",
    "            rel = src.relative_to(root)\n",
    "        except Exception:\n",
    "            rel = src.name\n",
    "\n",
    "        dst = dest / rel\n",
    "        if dst.exists():\n",
    "            if not OVERWRITE:\n",
    "                continue\n",
    "            if dst.is_dir():\n",
    "                shutil.rmtree(dst)\n",
    "            else:\n",
    "                dst.unlink()\n",
    "\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if src.is_dir():\n",
    "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "dt = time.time() - t0\n",
    "_p(f\"[DONE] elapsed: {dt:.1f}s\")\n",
    "_p(f\"Extracted names:  {len(names)}  -> {dest/'extracted_names.txt'}\")\n",
    "_p(f\"Matched gbk:      {len(matched)} -> {dest/'matched_region_files.abs.txt'}\")\n",
    "_p(f\"Kept gbk:         {len(kept)} -> {dest/'kept_region_files.abs.txt'}\")\n",
    "_p(f\"Copied targets:   {len(targets)} -> {dest/'kept_targets.abs.txt'}\")\n",
    "_p(f\"Unmatched names:  {len(unmatched)} -> {dest/'unmatched_names.txt'}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "0ea0aa15"
   },
   "source": [
    "#@title 8) Diagnostics (optional): explain why no GBKs passed the filter\n",
    "#@markdown Use this cell only if the previous filtering step yields zero hits. It summarizes reasons and writes a TSV report.\n",
    "\n",
    "\n",
    "import re, os\n",
    "from pathlib import Path\n",
    "\n",
    "DEST = \"/content/final-target\"  #@param {type:\"string\"}\n",
    "PROTO_GAP_MAX = 25000           #@param {type:\"integer\"}\n",
    "SHOW_TOP = 15\n",
    "\n",
    "dest = Path(DEST)\n",
    "matched_list = dest / \"matched_region_files.abs.txt\"\n",
    "out_tsv = dest / \"filter_diagnosis.tsv\"\n",
    "\n",
    "if not matched_list.exists():\n",
    "    raise FileNotFoundError(f\"Not found: {matched_list}\")\n",
    "\n",
    "files = [Path(x.strip()) for x in matched_list.read_text().splitlines() if x.strip()]\n",
    "print(f\"[INFO] diagnosing {len(files)} files ...\")\n",
    "\n",
    "# ---- reuse parsing logic (same as filter cell) ----\n",
    "_core_re = re.compile(r'/core_location=\"?\\[(\\d+):(\\d+)\\]\\([+-]\\)\"?')\n",
    "_prod_re = re.compile(r'/product=\"([^\"]+)\"')\n",
    "_pfam_re   = re.compile(r'/db_xref=\"(PF\\d+)\\.\\d+\"')\n",
    "_asdom_re  = re.compile(r'/aSDomain=\"([^\"]+)\"')\n",
    "_ps_re     = re.compile(r'/protein_start=\"(\\d+)\"')\n",
    "_pe_re     = re.compile(r'/protein_end=\"(\\d+)\"')\n",
    "\n",
    "def _is_qual(line: str) -> bool:\n",
    "    return line.startswith(\"                     \")  # 21 spaces\n",
    "\n",
    "def parse_protoclusters_core(lines):\n",
    "    clusters = []\n",
    "    i, n = 0, len(lines)\n",
    "    while i < n:\n",
    "        if lines[i].startswith(\"     protocluster\"):\n",
    "            cur = {\"product\": None, \"core_start\": None, \"core_end\": None}\n",
    "            i += 1\n",
    "            while i < n and _is_qual(lines[i]):\n",
    "                s = lines[i].strip()\n",
    "                m = _prod_re.search(s)\n",
    "                if m:\n",
    "                    cur[\"product\"] = m.group(1)\n",
    "                m = _core_re.search(s)\n",
    "                if m:\n",
    "                    a, b = int(m.group(1)), int(m.group(2))\n",
    "                    cur[\"core_start\"], cur[\"core_end\"] = (a, b) if a <= b else (b, a)\n",
    "                i += 1\n",
    "            if cur[\"product\"] and cur[\"core_start\"] is not None and cur[\"core_end\"] is not None:\n",
    "                clusters.append(cur)\n",
    "            continue\n",
    "        i += 1\n",
    "    return clusters\n",
    "\n",
    "def gap_half_open(a_start, a_end, b_start, b_end) -> int:\n",
    "    left = max(a_start, b_start)\n",
    "    right = min(a_end, b_end)\n",
    "    if left < right:\n",
    "        return 0\n",
    "    if a_end <= b_start:\n",
    "        return b_start - a_end\n",
    "    if b_end <= a_start:\n",
    "        return a_start - b_end\n",
    "    return 0\n",
    "\n",
    "def parse_unique_pfam_domains(lines):\n",
    "    domains = set()\n",
    "    i, n = 0, len(lines)\n",
    "    while i < n:\n",
    "        if lines[i].startswith(\"     PFAM_domain\"):\n",
    "            loc = lines[i][len(\"     PFAM_domain\"):].strip()\n",
    "            pfams = set()\n",
    "            asdom = \"\"\n",
    "            ps = None\n",
    "            pe = None\n",
    "            i += 1\n",
    "            while i < n and _is_qual(lines[i]):\n",
    "                s = lines[i].strip()\n",
    "                m = _pfam_re.search(s)\n",
    "                if m:\n",
    "                    pfams.add(m.group(1))\n",
    "                m = _asdom_re.search(s)\n",
    "                if m:\n",
    "                    asdom = m.group(1) or \"\"\n",
    "                m = _ps_re.search(s)\n",
    "                if m:\n",
    "                    ps = int(m.group(1))\n",
    "                m = _pe_re.search(s)\n",
    "                if m:\n",
    "                    pe = int(m.group(1))\n",
    "                i += 1\n",
    "            key = (loc, tuple(sorted(pfams)), asdom, ps, pe)\n",
    "            domains.add(key)\n",
    "            continue\n",
    "        i += 1\n",
    "    return domains\n",
    "\n",
    "def count_domains(domains):\n",
    "    pf00501_n = 0\n",
    "    amp_n = 0\n",
    "    cond_n = 0\n",
    "\n",
    "    for loc, pfams, asdom, ps, pe in domains:\n",
    "        pfams = set(pfams)\n",
    "        asdom_l = (asdom or \"\").lower()\n",
    "\n",
    "        if \"PF00501\" in pfams:\n",
    "            pf00501_n += 1\n",
    "        if asdom_l == \"amp-binding\":\n",
    "            amp_n += 1\n",
    "\n",
    "        is_cond = (\"PF00668\" in pfams) or (asdom_l in {\"condensation_lcl\",\"condensation_dcl\"})\n",
    "        if is_cond:\n",
    "            cond_n += 1\n",
    "\n",
    "    return pf00501_n, amp_n, cond_n\n",
    "\n",
    "def diagnose_one(p: Path):\n",
    "    try:\n",
    "        text = p.read_text(errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        return dict(file=str(p), readable=0, err=str(e))\n",
    "\n",
    "    step1 = 1 if re.search(r\"PKS_AT\", text, flags=re.I) else 0\n",
    "    step2 = 1 if re.search(r\"ketoacyl synthase\", text, flags=re.I) else 0\n",
    "    step4 = 1 if '/product=\"NRPS\"' in text else 0\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    domains = parse_unique_pfam_domains(lines)\n",
    "    pf00501_n, amp_n, cond_n = count_domains(domains)\n",
    "    step3_domains = 1 if ((pf00501_n >= 2 or amp_n >= 2) and cond_n >= 2) else 0\n",
    "\n",
    "    clusters = parse_protoclusters_core(lines)\n",
    "    nrps = [c for c in clusters if (c[\"product\"] or \"\").strip().upper() == \"NRPS\"]\n",
    "    hgl  = [c for c in clusters if \"HGLE-KS\" in (c[\"product\"] or \"\").strip().upper()]\n",
    "    has_nrps = 1 if nrps else 0\n",
    "    has_hgle = 1 if hgl else 0\n",
    "\n",
    "    min_gap = \"\"\n",
    "    step3_gap = 0\n",
    "    if nrps and hgl:\n",
    "        mg = None\n",
    "        for a in nrps:\n",
    "            for b in hgl:\n",
    "                g = gap_half_open(a[\"core_start\"], a[\"core_end\"], b[\"core_start\"], b[\"core_end\"])\n",
    "                mg = g if mg is None else min(mg, g)\n",
    "        min_gap = mg\n",
    "        step3_gap = 1 if (mg is not None and mg <= int(PROTO_GAP_MAX)) else 0\n",
    "\n",
    "    passed = 1 if (step1 and step2 and step3_domains and step3_gap and step4) else 0\n",
    "\n",
    "    reasons = []\n",
    "    if not step1: reasons.append(\"fail_step1(PKS_AT)\")\n",
    "    if not step2: reasons.append(\"fail_step2(ketoacyl_synthase)\")\n",
    "    if not step3_domains: reasons.append(f\"fail_step3_domains(PF00501={pf00501_n},AMP={amp_n},COND={cond_n})\")\n",
    "    if not has_nrps: reasons.append(\"fail_step3_gap(no_NRPS_protocluster)\")\n",
    "    if not has_hgle: reasons.append(\"fail_step3_gap(no_hglE-KS_protocluster)\")\n",
    "    if has_nrps and has_hgle and not step3_gap: reasons.append(f\"fail_step3_gap(min_gap={min_gap} > {PROTO_GAP_MAX})\")\n",
    "    if not step4: reasons.append('fail_step4(no_exact_/product=\"NRPS\")')\n",
    "    reason_str = \";\".join(reasons) if reasons else \"\"\n",
    "\n",
    "    return dict(\n",
    "        file=str(p),\n",
    "        readable=1,\n",
    "        step1=step1, step2=step2, step3_domains=step3_domains, step3_gap=step3_gap, step4=step4, passed=passed,\n",
    "        pf00501=pf00501_n, amp=amp_n, cond=cond_n,\n",
    "        protocluster_nrps=len(nrps), protocluster_hgle=len(hgl),\n",
    "        min_gap=min_gap,\n",
    "        reason=reason_str\n",
    "    )\n",
    "\n",
    "rows = [diagnose_one(p) for p in files]\n",
    "\n",
    "cols = [\n",
    "    \"file\",\"readable\",\"passed\",\n",
    "    \"step1\",\"step2\",\"step3_domains\",\"step3_gap\",\"step4\",\n",
    "    \"pf00501\",\"amp\",\"cond\",\n",
    "    \"protocluster_nrps\",\"protocluster_hgle\",\"min_gap\",\n",
    "    \"reason\"\n",
    "]\n",
    "out_lines = [\"\\t\".join(cols)]\n",
    "for r in rows:\n",
    "    out_lines.append(\"\\t\".join(str(r.get(c,\"\")) for c in cols))\n",
    "out_tsv.write_text(\"\\n\".join(out_lines) + \"\\n\", encoding=\"utf-8\")\n",
    "print(f\"[INFO] wrote: {out_tsv}\")\n",
    "\n",
    "total = len(rows)\n",
    "passed_n = sum(1 for r in rows if r.get(\"passed\")==1)\n",
    "print(f\"[SUMMARY] total={total}  passed={passed_n}\")\n",
    "\n",
    "def count_fail(key):\n",
    "    return sum(1 for r in rows if r.get(key)==0 and r.get(\"readable\")==1)\n",
    "\n",
    "print(\"[SUMMARY] fail counts (among readable files):\")\n",
    "print(\"  step1(PKS_AT):\", count_fail(\"step1\"))\n",
    "print(\"  step2(ketoacyl synthase):\", count_fail(\"step2\"))\n",
    "print(\"  step3_domains:\", count_fail(\"step3_domains\"))\n",
    "print(\"  step3_gap:\", count_fail(\"step3_gap\"))\n",
    "print(\"  step4(/product=\\\"NRPS\\\"):\", count_fail(\"step4\"))\n",
    "\n",
    "fails = [r for r in rows if r.get(\"readable\")==1 and r.get(\"passed\")==0]\n",
    "print(f\"\\n[EXAMPLES] showing up to {SHOW_TOP} fails:\")\n",
    "for r in fails[:SHOW_TOP]:\n",
    "    print(\"-\", Path(r[\"file\"]).name)\n",
    "    print(\"  \", r[\"reason\"])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "fa7f9055"
   },
   "source": [
    "#@title 9) Clinker: visualize only the kept GBKs (deduplicated)\n",
    "#@markdown Runs clinker on the GBKs that passed filtering and generates an interactive HTML and a session JSON.\n",
    "\n",
    "\n",
    "import os, subprocess, shutil, threading, http.server, socketserver, socket\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== config ======\n",
    "DEST = Path(globals().get(\"DEST\", \"final-target\")).resolve()\n",
    "KEEP_LIST = DEST / \"kept_region_files.abs.txt\"\n",
    "\n",
    "OUTDIR = DEST / \"clinker_kept_out\"\n",
    "INPUTS = OUTDIR / \"inputs\"\n",
    "PLOT_HTML = OUTDIR / \"clinker_kept.html\"\n",
    "SESSION_JSON = OUTDIR / \"clinker_kept.session.json\"\n",
    "\n",
    "JOBS = \"0\"\n",
    "FORCE = True\n",
    "MAX_CLUSTERS = 0\n",
    "# ====================\n",
    "\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"clinker\"])\n",
    "\n",
    "if not KEEP_LIST.exists():\n",
    "    raise FileNotFoundError(f\"Not found: {KEEP_LIST}\")\n",
    "\n",
    "# 1) read kept list + dedup\n",
    "raw = [x.strip() for x in KEEP_LIST.read_text().splitlines() if x.strip()]\n",
    "gbk_paths = [Path(x) for x in raw]\n",
    "gbk_paths = sorted({p.resolve() for p in gbk_paths if p.exists()})\n",
    "\n",
    "print(f\"[INFO] kept gbk files (raw lines) = {len(raw)}\")\n",
    "print(f\"[INFO] kept gbk files (exists+dedup) = {len(gbk_paths)}\")\n",
    "\n",
    "if not gbk_paths:\n",
    "    print(\"[INFO] kept list empty after dedup/existence check.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "if MAX_CLUSTERS and len(gbk_paths) > MAX_CLUSTERS:\n",
    "    gbk_paths = gbk_paths[:MAX_CLUSTERS]\n",
    "    print(f\"[INFO] capped to first {MAX_CLUSTERS} gbks\")\n",
    "\n",
    "# 2) rebuild inputs directory cleanly\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "if INPUTS.exists():\n",
    "    shutil.rmtree(INPUTS)\n",
    "INPUTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) isolate inputs: symlink (preferred) or copy\n",
    "kept_inputs = []\n",
    "src_seen = {}  # src_resolved -> dst list\n",
    "for i, src in enumerate(gbk_paths, start=1):\n",
    "    dst = INPUTS / f\"{i:05d}.{src.name}\"\n",
    "    try:\n",
    "        os.symlink(src, dst)\n",
    "    except Exception:\n",
    "        shutil.copy2(src, dst)\n",
    "    kept_inputs.append(dst)\n",
    "    src_seen.setdefault(str(src), []).append(str(dst))\n",
    "\n",
    "# 3.1 sanity: any source linked twice?\n",
    "dups = {s: ds for s, ds in src_seen.items() if len(ds) > 1}\n",
    "if dups:\n",
    "    print(\"[WARN] same source file appears multiple times in inputs (should not happen):\")\n",
    "    for s, ds in dups.items():\n",
    "        print(\"  SRC:\", s)\n",
    "        for d in ds:\n",
    "            print(\"    ->\", d)\n",
    "else:\n",
    "    print(\"[INFO] no duplicate sources in inputs.\")\n",
    "\n",
    "# 4) explicit list inputs/*.gbk (no shell glob)\n",
    "input_files = sorted(INPUTS.glob(\"*.gbk\"))\n",
    "print(f\"[INFO] input gbk files prepared = {len(input_files)}\")\n",
    "# 4.1 final dedup by realpath just in case\n",
    "real2file = {}\n",
    "for p in input_files:\n",
    "    rp = str(p.resolve())\n",
    "    real2file.setdefault(rp, []).append(str(p))\n",
    "dup_real = {rp: ps for rp, ps in real2file.items() if len(ps) > 1}\n",
    "if dup_real:\n",
    "    print(\"[WARN] duplicate realpaths among inputs (multiple links to same file):\")\n",
    "    for rp, ps in dup_real.items():\n",
    "        print(\"  REAL:\", rp)\n",
    "        for p in ps:\n",
    "            print(\"    ->\", p)\n",
    "\n",
    "# 5) run clinker with explicit argv list (no shell)\n",
    "cmd = [\"clinker\", *map(str, input_files), \"-p\", str(PLOT_HTML), \"-s\", str(SESSION_JSON), \"-j\", str(JOBS)]\n",
    "if FORCE:\n",
    "    cmd += [\"-f\"]\n",
    "\n",
    "print(\"[INFO] running clinker with N inputs =\", len(input_files))\n",
    "print(\"[INFO] cmd head:\", \" \".join(cmd[:8]), \"...\")\n",
    "subprocess.run(cmd, check=True)\n",
    "\n",
    "print(f\"[INFO] plot saved: {PLOT_HTML}\")\n",
    "print(f\"[INFO] session saved: {SESSION_JSON}\")\n",
    "\n",
    "# 6) display once with a free port (avoid 'address already in use')\n",
    "from google.colab import output\n",
    "from IPython.display import display\n",
    "\n",
    "def pick_free_port():\n",
    "    s = socket.socket()\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "PORT = pick_free_port()\n",
    "\n",
    "class QuietHandler(http.server.SimpleHTTPRequestHandler):\n",
    "    def log_message(self, format, *args):\n",
    "        pass\n",
    "\n",
    "def serve(port):\n",
    "    os.chdir(str(OUTDIR))\n",
    "    with socketserver.TCPServer((\"\", port), QuietHandler) as httpd:\n",
    "        httpd.serve_forever()\n",
    "\n",
    "t = threading.Thread(target=serve, args=(PORT,), daemon=True)\n",
    "t.start()\n",
    "\n",
    "display(output.serve_kernel_port_as_iframe(PORT, path=f\"/{PLOT_HTML.name}\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6300af5b"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "- NCBI BLAST URLAPI documentation (for programmatic BLAST queries).\n",
    "- antiSMASH for BGC prediction.\n",
    "- cblaster for local gene cluster searches.\n",
    "\n",
    "If you distribute results, please cite the upstream tools you used (antiSMASH, cblaster, etc.) according to their licenses and publication guidance.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09f0466485e642f9867d680e6854ec66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_364bc977888843d78cc86b6fabe14705",
       "IPY_MODEL_1cb7fc53359d4f85965d994a1c86410d",
       "IPY_MODEL_b2cfae048f454175906e3c5d8dcf445a"
      ],
      "layout": "IPY_MODEL_051713e22d004f8584c9d42c6a857f09"
     }
    },
    "364bc977888843d78cc86b6fabe14705": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da4a5e6623184bef9ec23891a9a69ef3",
      "placeholder": "​",
      "style": "IPY_MODEL_157b5e55672a4641b2c6cb829a083c5a",
      "value": "Downloading unique taxids: 100%"
     }
    },
    "1cb7fc53359d4f85965d994a1c86410d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9702c65af22d4b27b55d20c917c74444",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5ad773c9a0614b418fe77f7766f43636",
      "value": 1
     }
    },
    "b2cfae048f454175906e3c5d8dcf445a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d0221543b9c4e819ebfba1fbec2261a",
      "placeholder": "​",
      "style": "IPY_MODEL_9f028ae5b5b84982ade9d92abfa98ebe",
      "value": " 1/1 [00:01&lt;00:00,  1.15s/item]"
     }
    },
    "051713e22d004f8584c9d42c6a857f09": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da4a5e6623184bef9ec23891a9a69ef3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "157b5e55672a4641b2c6cb829a083c5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9702c65af22d4b27b55d20c917c74444": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ad773c9a0614b418fe77f7766f43636": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0d0221543b9c4e819ebfba1fbec2261a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f028ae5b5b84982ade9d92abfa98ebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
